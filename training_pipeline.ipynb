{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","colab":{"gpuType":"V5E1","provenance":[],"include_colab_link":true},"jupytext":{"formats":"ipynb,md:myst"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[],"dockerImageVersionId":31261,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/WilsonWang01/hands-on-gpt2/blob/main/training_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train a miniGPT language model with JAX","metadata":{"id":"3FaMXrpXDQyL"}},{"cell_type":"markdown","source":"<table class=\"tfo-notebook-buttons\" align=\"left\">\n  <td>\n    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.kaggle.com/static/images/logos/kaggle-logo-transparent-300.png\" height=\"32\" width=\"70\"/>Run in Kaggle</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://colab.research.google.com/github/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://github.com/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n  </td>\n</table>","metadata":{"id":"MGV1PVM5DQyS"}},{"cell_type":"markdown","source":"This tutorial demonstrates how to use JAX, [Flax NNX](http://flax.readthedocs.io) and [Optax](http://optax.readthedocs.io) for language model (pre)training using data and tensor [parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for [Single-Program Multi-Data](https://en.wikipedia.org/wiki/Single_program,_multiple_data)). It was originally inspired by the [Keras miniGPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/).\n\nHere, you will learn how to:\n\n- Define the miniGPT model with Flax and JAX automatic parallelism\n- Load and preprocess the dataset\n- Create the loss and training step functions\n- Train the model on TPUs on Kaggle or Google Colab\n- Profile for hyperparameter tuning\n\nIf you are new to JAX for AI, check out the [introductory tutorial](https://jax-ai-stack.readthedocs.io/en/latest/neural_net_basics.html), which covers neural network building with [Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html).","metadata":{"id":"NIOXoY1xgiww"}},{"cell_type":"markdown","source":"**Note:** If you are using [Kaggle](https://www.kaggle.com/), select the free TPU v5e-8 as the hardware accelerator. If you are using [Google Colab](https://colab.research.google.com/), select the free Google Cloud TPU v5e-1 as the hardware accelerator. You may also use Google Cloud TPUs.\n\nCheck the available JAX devices, or [`jax.Device`](https://jax.readthedocs.io/en/latest/_autosummary/jax.Device.html), with [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html). The output of the cell below will show a list of 8 (eight) devices.","metadata":{"id":"Rcji_799n4eA"}},{"cell_type":"code","source":"# Âº∫Âà∂ÂÆâË£ÖÈÄÇÈÖçÁöÑ 0.8.2‰ª•‰∏ä ÁâàÊú¨ÔºåÈÅøÂÖçÁâàÊú¨ÂÜ≤Á™Å\n!pip install \"jax[tpu]>=0.8.2\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qWm1-kAnKnl4","outputId":"add67bf6-2001-4c4d-c7f0-cbda7e2cdd8d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import jax\n# Ê£ÄÊü•ËÆæÂ§áÂàóË°®\nprint(jax.devices())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LS9sQEY3n0mB","outputId":"4b7a6921-7b10-4458-eec3-85a1a725aace","trusted":true,"execution":{"iopub.status.idle":"2026-01-31T17:00:57.163665Z","shell.execute_reply.started":"2026-01-31T17:00:46.955655Z","shell.execute_reply":"2026-01-31T17:00:57.162315Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1769878847.384461   53229 common_lib.cc:650] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:238\n","output_type":"stream"},{"name":"stdout","text":"[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=4, process_index=0, coords=(0,2,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(1,2,0), core_on_chip=0), TpuDevice(id=6, process_index=0, coords=(0,3,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,3,0), core_on_chip=0)]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### download web_novel dataset from dropbox","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport zipfile\n\n# 1. ‰∏ãËΩΩ GuoFeng Corpus (Dropbox)\nprint(\"‚¨áÔ∏è Downloading GuoFeng Webnovel Corpus...\")\n!wget -O webnovel_data.zip \"https://www.dropbox.com/scl/fo/dtrf3pe1vfbo5nse16648/AAZ5SFnuwohj7IJ2J-Q8zHs?rlkey=486vbn17qra1ez91btj0n4xu2&e=1&dl=1\"\n\n# 2. Ëß£Âéã\nprint(\"üìÇ Unzipping...\")\nwith zipfile.ZipFile(\"webnovel_data.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"webnovel_raw\")\n\n# 3. ËΩ¨Êç¢‰∏∫ JSONL\noutput_file = \"/kaggle/working/webnovel_train.jsonl\"\nsource_zh = None\nsource_en = None\n\n# Ëá™Âä®ÂØªÊâæËß£ÂéãÂêéËóèÂú®Ê∑±Â±ÇÁõÆÂΩïÈáåÁöÑÊñá‰ª∂\nfor root, dirs, files in os.walk(\"webnovel_raw\"):\n    if \"train.zh\" in files: source_zh = os.path.join(root, \"train.zh\")\n    if \"train.en\" in files: source_en = os.path.join(root, \"train.en\")\n\nif source_zh and source_en:\n    print(f\"‚úÖ Found source files:\\n  ZH: {source_zh}\\n  EN: {source_en}\")\n    \n    with open(source_zh, 'r', encoding='utf-8') as f_zh, \\\n         open(source_en, 'r', encoding='utf-8') as f_en, \\\n         open(output_file, 'w', encoding='utf-8') as f_out:\n        \n        count = 0\n        for line_zh, line_en in zip(f_zh, f_en):\n            zh_text = line_zh.strip()\n            en_text = line_en.strip()\n            if not zh_text or not en_text: continue\n            \n            # Ê†ºÂºèÔºö‰∏≠Êñá + Êç¢Ë°å + Ëã±Êñá\n            # ËøôÊ†∑ËÆ≠ÁªÉÂá∫Êù•ÁöÑÊ®°ÂûãÔºåÁªôÂÆÉ‰∏≠ÊñáÔºåÂÆÉÂ∞±‰ºöÈ¢ÑÊµãÂá∫Ëã±ÊñáÁøªËØë\n            record = { \"text\": f\"{zh_text}\\n{en_text}\" }\n            \n            f_out.write(json.dumps(record, ensure_ascii=False) + '\\n')\n            count += 1\n            \n    print(f\"üéâ Done! Processed {count} lines. File saved at: {output_file}\")\nelse:\n    print(\"‚ùå Error: Could not find train.zh or train.en files.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download & Process the Data for JAX\nimport os\nimport json\nimport zipfile\n# 1. Download the GuoFeng Corpus directly\n# We use the 'dl=1' flag to get the file directly\nprint(\"‚¨áÔ∏è Downloading GuoFeng Webnovel Corpus...\")\n!wget -O webnovel_data.zip \"https://www.dropbox.com/scl/fo/dtrf3pe1vfbo5nse16648/AAZ5SFnuwohj7IJ2J-Q8zHs?rlkey=486vbn17qra1ez91btj0n4xu2&e=1&dl=1\"\n# 2. Unzip\nprint(\"üìÇ Unzipping...\")\nwith zipfile.ZipFile(\"webnovel_data.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"webnovel_raw\")\n# 3. Convert to JSONL (JAX friendly format)\n# We will combine train.zh (Chinese) and train.en (English)\n# into a format where the model learns to translate or associate pairs.\nfinal_output_file = \"/content/webnovel_train.jsonl\"\nsource_zh = \"webnovel_raw/V1/TRAIN/train.zh\"\nsource_en = \"webnovel_raw/V1/TRAIN/train.en\"\n# Verify path logic (in case unzip structure varies)\n# Sometimes dropbox zips create a top-level folder. We search for the files.\nfound_zh = None\nfound_en = None\nfor root, dirs, files in os.walk(\"webnovel_raw\"):\n    if \"train.zh\" in files: found_zh = os.path.join(root, \"train.zh\")\n    if \"train.en\" in files: found_en = os.path.join(root, \"train.en\")\nif found_zh and found_en:\n    print(f\"‚úÖ Found source files:\\n  ZH: {found_zh}\\n  EN: {found_en}\")\n\n    print(\"üîÑ converting to JSONL...\")\n    with open(found_zh, 'r', encoding='utf-8') as f_zh, \\\n         open(found_en, 'r', encoding='utf-8') as f_en, \\\n         open(final_output_file, 'w', encoding='utf-8') as f_out:\n\n        count = 0\n        for line_zh, line_en in zip(f_zh, f_en):\n            zh_text = line_zh.strip()\n            en_text = line_en.strip()\n            if not zh_text or not en_text: continue\n\n            # --- CHOOSE YOUR TRAINING FORMAT HERE ---\n\n            # OPTION A: Translation (Instruction Tuning style)\n            # Use this if your model supports 'instruction' fields\n            # record = {\n            #     \"instruction\": \"Translate chinese to english\",\n            #     \"input\": zh_text,\n            #     \"output\": en_text\n            # }\n            # OPTION B: Raw Text (Pretraining style)\n            # Use this if you are doing standard Causal Language Modeling\n            # The model just sees the Chinese followed by English\n            record = {\n                \"text\": f\"{zh_text}\\n{en_text}\"\n            }\n\n            f_out.write(json.dumps(record, ensure_ascii=False) + '\\n')\n            count += 1\n\n    print(f\"üéâ Done! Created {final_output_file} with {count} examples.\")\nelse:\n    print(\"‚ùå Could not find train.zh or train.en inside the zip.\")","metadata":{"id":"2xkJpCFr-MZR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Import the necessary modules, including JAX NumPy, Flax NNX, Optax, Grain, pandas, and Tiktoken:","metadata":{"id":"sKE2uUafLobI"}},{"cell_type":"code","source":"# ‰∏ÄÊ¨°ÊÄßÂÆâË£ÖÊâÄÊúâÂèØËÉΩÁº∫Â§±ÁöÑÂ∫ì\n!pip install tiktoken grain-nightly flax optax","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BnoAQLrwSCo3","outputId":"c4117a5f-577f-48ee-8674-c4001f9c16b5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import jax\nimport jax.numpy as jnp\n\nfrom jax.sharding import Mesh, PartitionSpec as P, NamedSharding # For data and model parallelism (explained in more detail later)\nfrom jax.experimental import mesh_utils\n\nimport flax.nnx as nnx\nimport optax\n\nfrom dataclasses import dataclass\nimport grain.python as pygrain\nimport pandas as pd\nimport time","metadata":{"id":"MKYFNOhdLq98","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T17:00:57.970737Z","iopub.execute_input":"2026-01-31T17:00:57.971023Z","iopub.status.idle":"2026-01-31T17:00:57.974220Z","shell.execute_reply.started":"2026-01-31T17:00:57.971006Z","shell.execute_reply":"2026-01-31T17:00:57.973468Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Define the miniGPT model with Flax and JAX automatic parallelism\n\n### Leveraging JAX's data and tensor parallelism\n\nOne of the most powerful features of JAX is [device parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for SPMD.\n\n- The data parallelism technique enables, for example, the training data to run via multiple parts (this is called sharding) - batches - in parallel and simultaneously across different devices, such as GPUs and Google TPUs. This allows to use larger batch sizes to speed up training.\n- Tensor parallelism allows us to split the model parameter tensors across several devices (sharding model tensors).\n- You can learn more about the basics of JAX parallelism in more detail in the [Introduction to parallel programming](https://jax.readthedocs.io/en/latest/sharded-computation.html) on the JAX documentation site.\n\nIn this example, we'll utilize a 4-way data parallel and 2-way tensor parallel setup, which is aligned with Kaggle TPU v5e-8 or newer GCP TPUs chips.\n\nNote that as of October 2025, free-tier Colab only offers TPU v5e-1, which can no longer support SPMD.\n\n### jax.sharding.Mesh\n\nEarlier, we imported [`jax.sharding.Mesh`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh) - is a multidimensional NumPy array of JAX devices, where each axis of the mesh has a name, such as `'x'` or `'y'`. This will help encapsulate the information about the TPU resource organization for distributing computations across the devices.\n\nOur `Mesh` will have two arguments:\n- `devices`: This will take the value of [`jax.experimental.mesh_utils((4, 2))`](https://jax.readthedocs.io/en/latest/jax.experimental.mesh_utils.html), enabling us to build a device mesh. It is a NumPy ndarray with JAX devices (a list of devices from the JAX backend as obtained from [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html#jax.devices))..\n- `axis_names`, where:\n  - `batch`: 4 devices along the first axis - i.e. sharded into 4 - for data parallelism; and\n  - `model`: 2 devices along the second axis - i.e. sharded into 2 -  for tensor parallism\n\nThis matches the structure in the Kaggle TPU v5e setup.\n\nLet's instantiate `Mesh` as `mesh` and declare the TPU configuration to define how data and model parameters are distributed across the devices:","metadata":{"id":"rPyt7MV6prz1"}},{"cell_type":"code","source":"# Create a `Mesh` object representing TPU device arrangement.\n# For example, for Kaggle TPU v5e-8:\nif jax.device_count() == 8:\n    mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n\n    ### Alternatively, we could use the 8-way data parallelism with only one line of code change.\n    ### JAX enables quick experimentation with different partitioning strategies\n    ### like this. We will come back to this point at the end of this tutorial.\n    # mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))\n\n### For free-tier Colab TPU, which only has a single TPU core\nif jax.device_count() == 1:\n    mesh = Mesh(mesh_utils.create_device_mesh((1, 1)), (\"batch\", \"model\"))","metadata":{"id":"xuMlCK3Q8WJD","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T17:03:02.638569Z","iopub.execute_input":"2026-01-31T17:03:02.638851Z","iopub.status.idle":"2026-01-31T17:03:02.642976Z","shell.execute_reply.started":"2026-01-31T17:03:02.638831Z","shell.execute_reply":"2026-01-31T17:03:02.641996Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# ============================================================================\n# KAGGLE TPU Â§öËØ≠Ë®Ä TOKENIZER (Yi-1.5, 64K ËØçË°®)\n# ============================================================================\n# ËØçË°®Â§ßÂ∞è: 64,000 (ÈÄÇÂêà‰∏≠Ëã±ÊñáÊ∑∑ÂêàËÆ≠ÁªÉ)\n# Êù•Ê∫ê: 01-ai/Yi-1.5-6B (HuggingFace)\n# ============================================================================","metadata":{"id":"_ZKdhNo98NgG"}},{"cell_type":"code","source":"# ============================================================================\n# KAGGLE TPU Â§öËØ≠Ë®Ä TOKENIZER (Yi-1.5, 64K ËØçË°®)\n# ============================================================================\n# ËØçË°®Â§ßÂ∞è: 64,000 (ÈÄÇÂêà‰∏≠Ëã±ÊñáÊ∑∑ÂêàËÆ≠ÁªÉ)\n# Êù•Ê∫ê: 01-ai/Yi-1.5-6B (HuggingFace)\n# ============================================================================\n\n# Step 1: ÂÆâË£Ö‰æùËµñ (ËøêË°å‰∏ÄÊ¨°ÂêéÈáçÂêØsession)\n# !pip install transformers\n\nfrom typing import List, Optional, Union\n\n\nclass MultilingualTokenizer:\n    \"\"\"\n    Yi-1.5 tokenizer wrapper (64K vocab)\n    ÊîØÊåÅ‰∏≠Ëã±ÊñáÔºåtiktoken ÂÖºÂÆπ API\n    \"\"\"\n    \n    def __init__(self, model_name: str = \"01-ai/Yi-1.5-6B\"):\n        \"\"\"\n        Args:\n            model_name: HuggingFace Ê®°ÂûãIDÔºåÊé®ËçêÈÄâÈ°π:\n                - \"01-ai/Yi-1.5-6B\" (ÈªòËÆ§): 64KËØçË°®Ôºå‰∏≠Ëã±Êñá‰ºòÁßÄ\n                - \"baichuan-inc/Baichuan2-7B-Base\": 125KËØçË°®\n        \"\"\"\n        from transformers import AutoTokenizer\n        \n        self._tokenizer = AutoTokenizer.from_pretrained(\n            model_name, \n            trust_remote_code=True,\n            use_fast=True\n        )\n        \n        self._eot_token = self._tokenizer.eos_token_id\n        self._bos_token = self._tokenizer.bos_token_id\n        self._pad_token = self._tokenizer.pad_token_id if self._tokenizer.pad_token_id is not None else 0\n        \n        # TPUÂØπÈΩê: ËØçË°®ÂøÖÈ°ªËÉΩË¢´128Êï¥Èô§\n        raw_vocab = len(self._tokenizer)\n        self._padded_vocab = ((raw_vocab // 128) + 1) * 128 if raw_vocab % 128 != 0 else raw_vocab\n    \n    @property\n    def n_vocab(self) -> int:\n        \"\"\"ÂéüÂßãËØçË°®Â§ßÂ∞è\"\"\"\n        return len(self._tokenizer)\n    \n    @property\n    def padded_vocab_size(self) -> int:\n        \"\"\"TPUÂØπÈΩêÂêéÁöÑËØçË°®Â§ßÂ∞è\"\"\"\n        return self._padded_vocab\n    \n    @property\n    def eot_token(self) -> int:\n        \"\"\"ÁªìÊùüÊ†áËÆ∞ID\"\"\"\n        return self._eot_token\n    \n    @property\n    def bos_token(self) -> int:\n        \"\"\"ÂºÄÂßãÊ†áËÆ∞ID\"\"\"\n        return self._bos_token\n    \n    @property\n    def pad_token(self) -> int:\n        \"\"\"Â°´ÂÖÖÊ†áËÆ∞ID\"\"\"\n        return self._pad_token\n    \n    def encode(self, text: str, allowed_special: Optional[set] = None, add_special_tokens: bool = False) -> List[int]:\n        \"\"\"ÁºñÁ†ÅÊñáÊú¨‰∏∫token IDs\"\"\"\n        return self._tokenizer.encode(text, add_special_tokens=add_special_tokens)\n    \n    def decode(self, tokens: Union[List[int], int]) -> str:\n        \"\"\"Ëß£Á†Åtoken IDs‰∏∫ÊñáÊú¨\"\"\"\n        if isinstance(tokens, int):\n            tokens = [tokens]\n        return self._tokenizer.decode(tokens, skip_special_tokens=True)\n\n\ndef get_tokenizer(model_name: str = \"01-ai/Yi-1.5-6B\") -> MultilingualTokenizer:\n    \"\"\"ÂàõÂª∫tokenizer (ÈªòËÆ§Yi-1.5, 64KËØçË°®)\"\"\"\n    return MultilingualTokenizer(model_name)\n\n\n# ============================================================================\n# ‰ΩøÁî®Á§∫‰æã / USAGE\n# ============================================================================\nif __name__ == \"__main__\" or True:  # Always run in notebook\n    print(\"=\" * 50)\n    print(\"Initializing Yi-1.5 Tokenizer (64K vocab)...\")\n    \n    tokenizer = get_tokenizer()\n    \n    print(f\"‚úì Raw vocab size: {tokenizer.n_vocab:,}\")\n    print(f\"‚úì Padded vocab (TPU): {tokenizer.padded_vocab_size:,}\")\n    print(f\"‚úì Divisible by 128: {tokenizer.padded_vocab_size % 128 == 0}\")\n    \n    # ÊµãËØïÁºñÁ†Å\n    test_texts = [\n        \"ËøôÊòØ‰∏≠ÊñáÊµãËØï\",\n        \"English test\",\n        \"‰∏≠Ëã±Ê∑∑ÂêàMixedÊñáÊú¨\"\n    ]\n    \n    print(\"\\n--- Tokenization Test ---\")\n    for text in test_texts:\n        ids = tokenizer.encode(text)\n        print(f\"'{text}' ‚Üí {len(ids)} tokens\")\n\n\n# ============================================================================\n# Âú®Ê®°ÂûãÈÖçÁΩÆ‰∏≠‰ΩøÁî®:\n# vocab_size = tokenizer.padded_vocab_size  # 64,128\n# model = MiniGPT(vocab_size=vocab_size, ...)\n# ============================================================================\n","metadata":{"id":"iWbkk1V7-Isg","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T17:01:32.026156Z","iopub.execute_input":"2026-01-31T17:01:32.026478Z","iopub.status.idle":"2026-01-31T17:01:37.667443Z","shell.execute_reply.started":"2026-01-31T17:01:32.026448Z","shell.execute_reply":"2026-01-31T17:01:37.666163Z"}},"outputs":[{"name":"stdout","text":"==================================================\nInitializing Yi-1.5 Tokenizer (64K vocab)...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/torch_xla/__init__.py:258: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"‚úì Raw vocab size: 63,992\n‚úì Padded vocab (TPU): 64,000\n‚úì Divisible by 128: True\n\n--- Tokenization Test ---\n'ËøôÊòØ‰∏≠ÊñáÊµãËØï' ‚Üí 3 tokens\n'English test' ‚Üí 2 tokens\n'‰∏≠Ëã±Ê∑∑ÂêàMixedÊñáÊú¨' ‚Üí 6 tokens\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"To leverage model parallelism, we need to instruct the JAX compiler how to shard the model tensors across the TPU devices. Earlier, we also imported [`jax.sharding.PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) and [`jax.sharding.NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding):\n- [`PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) (using alias `P`) defines how tensors are sharded across the devices in our `Mesh`. Its elements describe how an input dimension is partitioned across mesh dimensions. For example, in `PartitionSpec('x', 'y')` the first dimension of data is sharded across `x` axis of the mesh, and the second one - across the `y` axis.\n  - We'll use `PartitionSpec` to describe how to shard a tensor across, for example, the `model` axis or be replicated on other dimensions (which is denoted by `None`).\n- [`NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding) is a (`Mesh`, `PartitionSpec`) pair that describes how to shard a model tensor across our `mesh`.\n- We combine `Mesh` (the TPU resources) with `PartitionSpec` and create a `NamedSharding`, which instructs how to shard each model tensor across the TPU devices.\n\nAdditionally, we'll use Flax NNX's [`flax.nnx.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/spmd.html#flax.nnx.with_partitioning) to let each model layer know that the model weights or tensors need to be sharded according to our specification. We need to do this for every tensor/layer in the model.\n- `nnx.with_partitioning` will take two arguments, such as the `initializer` (such as [`flax.nnx.initializers.xavier_uniform`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.xavier_uniform) and [`flax.nnx.initializers.zeros_init`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.zeros_init)) and `sharding` (e.g. `NamedSharding(Mesh, PartitionSpec)` or `NamedSharding(mesh, P('model')` in our case).","metadata":{"id":"0XHQ0BQ9-KIj"}},{"cell_type":"code","source":"# --- 1. Embedding Layer ---\nclass TokenAndPositionEmbedding(nnx.Module):\n    def __init__(self, maxlen, vocab_size, embed_dim, rngs):\n        self.token_embed = nnx.Embed(\n            num_embeddings=vocab_size,\n            features=embed_dim,\n            embedding_init=nnx.with_partitioning(nnx.initializers.normal(stddev=0.02), P(None, 'model')),\n            rngs=rngs,\n        )\n        self.position_embed = nnx.Embed(\n            num_embeddings=maxlen,\n            features=embed_dim,\n            embedding_init=nnx.with_partitioning(nnx.initializers.normal(stddev=0.02), P(None, 'model')),\n            rngs=rngs,\n        )\n    def __call__(self, x):\n        positions = jnp.arange(0, x.shape[-1])\n        return self.token_embed(x) + self.position_embed(positions)\n# --- 2. Transformer Block ---\nclass TransformerBlock(nnx.Module):\n    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n        self.mha = nnx.MultiHeadAttention(\n            num_heads=num_heads,\n            in_features=embed_dim,\n            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), P(None, 'model')),\n            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n            rngs=rngs,\n        )\n        self.dropout1 = nnx.Dropout(rate=rate)\n        self.layer_norm1 = nnx.LayerNorm(\n            epsilon=1e-6,\n            num_features=embed_dim,\n            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), P('model')),\n            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n            rngs=rngs,\n        )\n        self.linear1 = nnx.Linear(\n            in_features=embed_dim,\n            out_features=ff_dim,\n            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), P(None, 'model')),\n            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n            rngs=rngs,\n        )\n        self.dropout2 = nnx.Dropout(rate=rate)\n        self.linear2 = nnx.Linear(\n            in_features=ff_dim,\n            out_features=embed_dim,\n            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), P(None, 'model')),\n            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n            rngs=rngs,\n        )\n        self.layer_norm2 = nnx.LayerNorm(\n            epsilon=1e-6,\n            num_features=embed_dim,\n            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), P('model')),\n            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n            rngs=rngs,\n        )\n    # Added rngs=None\n    def __call__(self, x, mask=None, deterministic=False, rngs=None):\n        x_norm = self.layer_norm1(x)\n        # Pass rngs to MHA and Dropout\n        x_mha = self.mha(x_norm, mask=mask, decode=False, deterministic=deterministic, rngs=rngs) \n        x_mha = self.dropout1(x_mha, deterministic=deterministic, rngs=rngs)\n        x = x + x_mha \n        x_norm = self.layer_norm2(x)\n        x_ff = self.linear1(x_norm)\n        x_ff = nnx.gelu(x_ff)\n        x_ff = self.dropout2(x_ff, deterministic=deterministic, rngs=rngs)\n        x_ff = self.linear2(x_ff)\n        x = x + x_ff \n        return x\n# --- 3. MiniGPT ---\nclass MiniGPT(nnx.Module):\n    def __init__(self, maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks, *, rngs: nnx.Rngs):\n        self.embedding_layer = TokenAndPositionEmbedding(\n            maxlen, vocab_size, embed_dim, rngs=rngs\n        )\n        self.transformer_blocks = nnx.List([\n            TransformerBlock(embed_dim, num_heads, feed_forward_dim, rngs=rngs) \n            for _ in range(num_transformer_blocks)\n        ])\n        self.output_layer = nnx.Linear(\n            in_features=embed_dim,\n            out_features=vocab_size,\n            kernel_init=nnx.with_partitioning(\n                nnx.initializers.xavier_uniform(), \n                P(None, 'model')\n            ),\n            bias_init=nnx.with_partitioning(\n                nnx.initializers.zeros_init(), \n                P('model') \n            ),\n            rngs=rngs\n        )\n    # propagate rngs\n    def __call__(self, x, mask=None, deterministic=False, rngs=None):\n        x = self.embedding_layer(x)\n        for block in self.transformer_blocks:\n            x = block(x, mask=mask, deterministic=deterministic, rngs=rngs)\n        x = self.output_layer(x)\n        return x\n    def generate_token(self, input_ids):\n        logits = self(input_ids, deterministic=True)\n        return logits[0, -1, :] \n    def generate_text(self, max_tokens, start_tokens):\n        tokens = list(start_tokens)\n        for _ in range(max_tokens - len(start_tokens)):\n            input_ids = jnp.array([tokens])\n            logits = self(input_ids, deterministic=True)\n            next_token = jnp.argmax(logits[0, -1, :]).item()\n            tokens.append(next_token)\n            if next_token == tokenizer.eot_token:\n                break\n        return tokenizer.decode(tokens)","metadata":{"id":"z0p-IHurrB9i","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T17:02:50.684830Z","iopub.execute_input":"2026-01-31T17:02:50.685080Z","iopub.status.idle":"2026-01-31T17:02:50.697643Z","shell.execute_reply.started":"2026-01-31T17:02:50.685063Z","shell.execute_reply":"2026-01-31T17:02:50.696650Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"Set some hyperparameters.","metadata":{"id":"igX_eoGNMTGR"}},{"cell_type":"code","source":"vocab_size = tokenizer.padded_vocab_size  # Â∑≤ÁªèÂØπÈΩêÔºåÁõ¥Êé•Áî®\nprint(f\"Vocab: {vocab_size}\")  # Â∫îËØ•ÊòØ 64,000","metadata":{"id":"GRhiDsCrMZRp","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T17:02:42.947583Z","iopub.execute_input":"2026-01-31T17:02:42.947824Z","iopub.status.idle":"2026-01-31T17:02:42.951669Z","shell.execute_reply.started":"2026-01-31T17:02:42.947806Z","shell.execute_reply":"2026-01-31T17:02:42.950549Z"}},"outputs":[{"name":"stdout","text":"Vocab: 64000\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Ë∂ÖÂèÇÊï∞ÈÖçÁΩÆ","metadata":{}},{"cell_type":"code","source":"# (Ë∂ÖÂèÇÊï∞ÈÖçÁΩÆ)\nvocab_size = tokenizer.padded_vocab_size\nnum_transformer_blocks = 8\nmaxlen = 256\nembed_dim = 256\nnum_heads = 8\nfeed_forward_dim = 256\nbatch_size = 144 * jax.device_count() / 2\nif jax.device_count() == 1:\n    batch_size = 144\nnum_epochs = 1\ntop_k = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T17:02:31.187236Z","iopub.execute_input":"2026-01-31T17:02:31.187557Z","iopub.status.idle":"2026-01-31T17:02:31.191504Z","shell.execute_reply.started":"2026-01-31T17:02:31.187540Z","shell.execute_reply":"2026-01-31T17:02:31.190455Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Loading and preprocessing the data\n\nData loading and preprocessing with [Grain](https://github.com/google/grain).","metadata":{"id":"mI1ci-HyMspJ"}},{"cell_type":"code","source":"import json\n@dataclass\nclass TextDataset:\n    data: list\n    maxlen: int\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx: int):\n        # Use Tiktoken for tokenization\n        # Note: We append <|endoftext|> to clearly mark end of sample\n        text = self.data[idx]\n        encoding = tokenizer.encode(text)\n        \n        # Add EOS token manually if not present\n        if encoding[-1] != tokenizer.eot_token:\n            encoding.append(tokenizer.eot_token)\n        # Truncate and Pad\n        encoding = encoding[:self.maxlen] \n        padded = encoding + [0] * (self.maxlen - len(encoding))\n        return padded\ndef load_and_preprocess_data(file_path, batch_size, maxlen):\n    print(f\"üìñ Loading data from {file_path}...\")\n    \n    # Ensure batch_size is an Integer\n    batch_size = int(batch_size) \n    \n    data = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            if line.strip():\n                item = json.loads(line)\n                if 'text' in item:\n                    data.append(item['text'])\n    \n    print(f\"‚úÖ Loaded {len(data)} examples.\")\n    \n    dataset = TextDataset(data, maxlen)\n    sampler = pygrain.IndexSampler(\n        len(dataset),\n        shuffle=True,\n        seed=42,\n        shard_options=pygrain.NoSharding(),\n        num_epochs=num_epochs,\n    )\n    dl = pygrain.DataLoader(\n        data_source=dataset,\n        sampler=sampler,\n        # Fix is here: explicitly using the integer variable\n        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n    )\n    return dl\n# Re-run the data loading line right after defining this function\ntext_dl = load_and_preprocess_data('/kaggle/working/webnovel_train.jsonl', batch_size, maxlen)","metadata":{"id":"rGUFsn1GMuzh","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T17:03:26.878519Z","iopub.execute_input":"2026-01-31T17:03:26.878908Z","iopub.status.idle":"2026-01-31T17:03:31.523530Z","shell.execute_reply.started":"2026-01-31T17:03:26.878879Z","shell.execute_reply":"2026-01-31T17:03:31.522385Z"}},"outputs":[{"name":"stdout","text":"üìñ Loading data from /kaggle/working/webnovel_train.jsonl...\n‚úÖ Loaded 1920191 examples.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Defining the loss function and training step function","metadata":{"id":"BKVSD8KSM1um"}},{"cell_type":"code","source":"# Updated Loss Function to accept RNGs\ndef loss_fn(model, batch, rngs):\n    # Pass 'rngs' to the model call\n    logits = model(batch[0], rngs=rngs, deterministic=False) \n    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n    return loss, logits\n@nnx.jit\ndef train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch, dropout_key):\n    # Create the NNX RNG stream from the JAX key\n    dropout_rngs = nnx.Rngs(dropout=dropout_key)\n    \n    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n    \n    # Pass 'dropout_rngs' as the 3rd argument to matches loss_fn(model, batch, rngs)\n    (loss, logits), grads = grad_fn(model, batch, dropout_rngs)\n    \n    metrics.update(loss=loss, logits=logits, lables=batch[1])\n    optimizer.update(model, grads)","metadata":{"id":"8rRuTmABNV4b","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T17:03:38.801181Z","iopub.execute_input":"2026-01-31T17:03:38.801458Z","iopub.status.idle":"2026-01-31T17:03:38.806353Z","shell.execute_reply.started":"2026-01-31T17:03:38.801439Z","shell.execute_reply":"2026-01-31T17:03:38.805289Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Training the model\n\nStart training. It takes ~50 minutes on Colab.\n\nNote that for data parallel, we are sharding the training data along the `batch` axis using `jax.device_put` with `NamedSharding`.\n\nWe are also using the `jax.vmap` transformation to produce the target sequences faster.","metadata":{"id":"5um2vkeUNckm"}},{"cell_type":"code","source":"with mesh:\n    model = create_model(rngs=nnx.Rngs(0))\n    optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\nmetrics = nnx.MultiMetric(\n    loss=nnx.metrics.Average(\"loss\"),\n)\n# Initialize Main Random Key\nrng = jax.random.PRNGKey(0)\nstart_prompt = \"Once upon a time\"\nstart_tokens = tokenizer.encode(start_prompt)[:maxlen]\nprint(\"Initial generated text (Untrained):\")\ngenerated_text = model.generate_text(maxlen, start_tokens)\nprint(generated_text) # Print simple output\nmetrics_history = {\n    \"train_loss\": [],\n}\nprep_target_batch = jax.vmap(\n    lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0])))\n)\nstep = 0\nfor epoch in range(num_epochs):\n    start_time = time.time()\n    for batch in text_dl:\n        if len(batch) % len(jax.devices()) != 0:\n            continue\n            \n        input_batch = jnp.array(jnp.array(batch).T)\n        target_batch = prep_target_batch(input_batch)\n        \n        # 1. NEW: Split the key for this step\n        rng, dropout_key = jax.random.split(rng)\n        \n        # 2. NEW: Pass dropout_key to train_step\n        train_step(\n            model,\n            optimizer,\n            metrics,\n            jax.device_put(\n                (input_batch, target_batch), NamedSharding(mesh, P(\"batch\", None))\n            ),\n            dropout_key # <--- Added key here\n        )\n        if (step + 1) % 1 == 0: # Log every 10 steps\n            print(\"hi\")\n            for metric, value in metrics.compute().items():\n                metrics_history[f\"train_{metric}\"].append(value)\n            metrics.reset()\n            elapsed_time = time.time() - start_time\n            print(\n                f\"\\nStep {step + 1}, Loss: {metrics_history['train_loss'][-1]:.4f}, Time: {elapsed_time:.2f}s\"\n            )\n            # Re-generate text to see progress\n            print(\"Generated text:\")\n            print(model.generate_text(maxlen, start_tokens))\n            \n            start_time = time.time()\n        step += 1\n# Final text generation\nprint(\"Final generated text:\")\nprint(model.generate_text(maxlen, start_tokens))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ysl6CsfENeJN","outputId":"5624ab09-3c78-4732-873c-d5c20299df25","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T17:03:47.712845Z","iopub.execute_input":"2026-01-31T17:03:47.713131Z","iopub.status.idle":"2026-01-31T17:03:47.876729Z","shell.execute_reply.started":"2026-01-31T17:03:47.713112Z","shell.execute_reply":"2026-01-31T17:03:47.875453Z"}},"outputs":[{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mesh:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     model = \u001b[43mcreate_model\u001b[49m(rngs=nnx.Rngs(\u001b[32m0\u001b[39m))\n\u001b[32m      3\u001b[39m     optimizer = nnx.Optimizer(model, optax.adam(\u001b[32m1e-3\u001b[39m), wrt=nnx.Param)\n\u001b[32m      4\u001b[39m metrics = nnx.MultiMetric(\n\u001b[32m      5\u001b[39m     loss=nnx.metrics.Average(\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      6\u001b[39m )\n","\u001b[31mNameError\u001b[39m: name 'create_model' is not defined"],"ename":"NameError","evalue":"name 'create_model' is not defined","output_type":"error"}],"execution_count":13},{"cell_type":"markdown","source":"Visualize the training loss.","metadata":{"id":"thaLs6TD0lt5"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(metrics_history['train_loss'])\nplt.title('Training Loss')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"B6Eg1Cz2y_iP","outputId":"5da679f6-44de-443d-e73f-a6c35909a089","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As you can see, the model goes from generating completely random words at the beginning to generating sensible tiny stories at the end of the training. So essentially we have pretrained a small LLM to write tiny stories for us.","metadata":{"id":"WB-ExEt1Zl1C"}},{"cell_type":"markdown","source":"## Saving the checkpoint\n\nSave the model checkpoint.","metadata":{"id":"soPqiR1JNmjf"}},{"cell_type":"code","source":"import orbax.checkpoint as orbax\n\nstate = nnx.state(model)\n\ncheckpointer = orbax.PyTreeCheckpointer()\ncheckpointer.save('/content/save', args=orbax.args.PyTreeSave(state), force=True)\n\n# Make sure the files are there\n!ls /content/save/","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EkoFGCgSZ1yz","outputId":"3467b8ba-ce05-42f0-fb89-75922cc91e31","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Profiling for hyperparameter tuning\n\n**Note:** this section assume multiple TPU cores. Free-tier Colab TPU v5e-1 cannot run here.","metadata":{"id":"3813cbf2"}},{"cell_type":"code","source":"!pip install -Uq tensorboard-plugin-profile tensorflow tensorboard","metadata":{"id":"b5d933c6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load the tensorboard colab extension.","metadata":{"id":"2ac5fc4d"}},{"cell_type":"code","source":"%load_ext tensorboard","metadata":{"id":"74f0c212","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we're going to be running this model a number of times, we need some scaffolding to more easily compare our work. For a baseline, we'll need to perform some warmup to guarantee that our code is JIT'd and that our TPUs are warm. For improved comparability, we'll only start tracing after we've finished warmup.","metadata":{"id":"17c6131f"}},{"cell_type":"code","source":"trace_dir = \"/tmp/jax-trace/\"\n\ndef loop_step(batch, step):\n    input_batch = jnp.array(jnp.array(batch).T)\n    target_batch = prep_target_batch(input_batch)\n    train_step(model, optimizer, metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n\ndef generate_trace():\n    tracing_steps = 30\n    warmup_steps = 5\n    for current_step in range(warmup_steps + tracing_steps):\n        if current_step == warmup_steps:\n            jax.profiler.start_trace(trace_dir)\n        with jax.profiler.StepTraceAnnotation(\"train\", step_num=current_step):\n            batch = next(text_dl)\n            loop_step(batch, current_step)\n\n    jax.profiler.stop_trace()","metadata":{"id":"ddfd576e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we'll perform some traces to compare results of different batch sizes. This will take several minutes as we need to reprocess our input data to prepare new batches each time.","metadata":{"id":"de70f5b7"}},{"cell_type":"code","source":"trace_dir = \"/tmp/jax-trace-batch-comparison/\"\n\nbatch_size = 64\ntext_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\ngenerate_trace()\n\nbatch_size = 256\ntext_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\ngenerate_trace()","metadata":{"id":"bc9452a6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Run Tensorboard with the Profiler Plugin to compare our runs. Runs are listed in order from newest to oldest, so the top run in the list will be have `batch_size = 256`.\n\nThe key metrics to focus on here for this hyperparameter are FLOPS Utilization and Average Step Time.\n\nIn general, we want to maximize FLOPS Utilization while minimizing the step time per training example. In this case, we can see that increasing the batch size from 64 -> 256 achieves both of those. FLOPS increases from 16% to 27%. Average Step Time increase from 100ms to 260ms, however we increased our batch size by 300%. This means we move from 1.5ms per training example to 1.02ms per training example.","metadata":{"id":"ea379965"}},{"cell_type":"code","source":"%tensorboard --logdir=$trace_dir","metadata":{"id":"b86c565a","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we can explore alternative parallelism methods. In cell #4, we used 4-way data parallel and 2-way tensor parallel. 8-way data parallel is another popular way. Let's compare results between them. To switch to 8-way data parallel, we'll replace the `Mesh` definition with:\n\n`mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))`\n\nJAX will automatically figure out how to shard the model and data to use the new partition strategy and nothing else need to be done. Re-connect the TPU runtime and run it again to see how it runs.\n\nHow simple and powerful is this! And that's the beauty of JAX automatic parallelism.","metadata":{"id":"657967a5"}},{"cell_type":"code","source":"trace_dir = \"/tmp/jax-trace-parallelism-comparison/\"\n\nmesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\ngenerate_trace()\n\nmesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))\ngenerate_trace()","metadata":{"id":"80daa8dc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Once again we'll run tensorboard.\n\nLooking at the results, we see that the step times are nearly the same, however the FLOPS Utilization is at 13% for 8-way data parallelism compared to 27% or 4-way data parallelism.\n\nBy looking at the Trace Viewer tool and looking under each TPU's ops, we can see that the TPUs spend a large amount of time idle while waiting for the host, as well as spending a good amount of time in `reduce_sum` operations.","metadata":{"id":"ad96e72b"}},{"cell_type":"code","source":"%tensorboard --logdir=$trace_dir","metadata":{"id":"780e9c72","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"By changing hyperparameters and comparing profiles, we're able to gain significant insights into our bottlenecks and limitations. These are just two examples of hyperparameters to tune, but plenty more of them will have significant effects on training speed and resource utilization.","metadata":{"id":"deca486e"}}]}