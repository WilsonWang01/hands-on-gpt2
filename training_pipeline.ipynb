{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilsonWang01/hands-on-gpt2/blob/main/training_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FaMXrpXDQyL"
      },
      "source": [
        "# Train a miniGPT language model with JAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGV1PVM5DQyS"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.kaggle.com/static/images/logos/kaggle-logo-transparent-300.png\" height=\"32\" width=\"70\"/>Run in Kaggle</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIOXoY1xgiww"
      },
      "source": [
        "This tutorial demonstrates how to use JAX, [Flax NNX](http://flax.readthedocs.io) and [Optax](http://optax.readthedocs.io) for language model (pre)training using data and tensor [parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for [Single-Program Multi-Data](https://en.wikipedia.org/wiki/Single_program,_multiple_data)). It was originally inspired by the [Keras miniGPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/).\n",
        "\n",
        "Here, you will learn how to:\n",
        "\n",
        "- Define the miniGPT model with Flax and JAX automatic parallelism\n",
        "- Load and preprocess the dataset\n",
        "- Create the loss and training step functions\n",
        "- Train the model on TPUs on Kaggle or Google Colab\n",
        "- Profile for hyperparameter tuning\n",
        "\n",
        "If you are new to JAX for AI, check out the [introductory tutorial](https://jax-ai-stack.readthedocs.io/en/latest/neural_net_basics.html), which covers neural network building with [Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zMsOIc7ouCO",
        "outputId": "2f1152ba-dd49-4e53-acfa-5a4bf20939ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.0/101.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m138.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m585.6/585.6 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.5/180.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m171.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.7/79.7 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq tiktoken jax-ai-stack[grain] matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcji_799n4eA"
      },
      "source": [
        "**Note:** If you are using [Kaggle](https://www.kaggle.com/), select the free TPU v5e-8 as the hardware accelerator. If you are using [Google Colab](https://colab.research.google.com/), select the free Google Cloud TPU v5e-1 as the hardware accelerator. You may also use Google Cloud TPUs.\n",
        "\n",
        "Check the available JAX devices, or [`jax.Device`](https://jax.readthedocs.io/en/latest/_autosummary/jax.Device.html), with [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html). The output of the cell below will show a list of 8 (eight) devices."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å¼ºåˆ¶å®‰è£…é€‚é…çš„ 0.8.0 ç‰ˆæœ¬ï¼Œé¿å…ç‰ˆæœ¬å†²çª\n",
        "!pip install \"jax[tpu]==0.8.0\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWm1-kAnKnl4",
        "outputId": "add67bf6-2001-4c4d-c7f0-cbda7e2cdd8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
            "Collecting jax==0.8.0 (from jax[tpu]==0.8.0)\n",
            "  Downloading jax-0.8.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jaxlib<=0.8.0,>=0.8.0 (from jax==0.8.0->jax[tpu]==0.8.0)\n",
            "  Downloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax==0.8.0->jax[tpu]==0.8.0) (0.5.4)\n",
            "Requirement already satisfied: numpy>=2.0 in /usr/local/lib/python3.12/dist-packages (from jax==0.8.0->jax[tpu]==0.8.0) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax==0.8.0->jax[tpu]==0.8.0) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax==0.8.0->jax[tpu]==0.8.0) (1.16.3)\n",
            "Collecting libtpu==0.0.24.* (from jax[tpu]==0.8.0)\n",
            "  Downloading libtpu-0.0.24-cp312-cp312-manylinux_2_31_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from jax[tpu]==0.8.0) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]==0.8.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]==0.8.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]==0.8.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]==0.8.0) (2026.1.4)\n",
            "Downloading jax-0.8.0-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libtpu-0.0.24-cp312-cp312-manylinux_2_31_x86_64.whl (156.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.8.0-cp312-cp312-manylinux_2_27_x86_64.whl (79.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.7/79.7 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libtpu, jaxlib, jax\n",
            "  Attempting uninstall: libtpu\n",
            "    Found existing installation: libtpu 0.0.17\n",
            "    Uninstalling libtpu-0.0.17:\n",
            "      Successfully uninstalled libtpu-0.0.17\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.7.2\n",
            "    Uninstalling jaxlib-0.7.2:\n",
            "      Successfully uninstalled jaxlib-0.7.2\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.7.2\n",
            "    Uninstalling jax-0.7.2:\n",
            "      Successfully uninstalled jax-0.7.2\n",
            "Successfully installed jax-0.8.0 jaxlib-0.8.0 libtpu-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS9sQEY3n0mB",
        "outputId": "4b7a6921-7b10-4458-eec3-85a1a725aace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "# æ£€æŸ¥è®¾å¤‡åˆ—è¡¨\n",
        "print(jax.devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHzJ_bokoovZ"
      },
      "source": [
        "Get the [TinyStories dataset from Hugging Face](https://huggingface.co/datasets/roneneldan/TinyStories). We only use the training split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUjQsgQEmI1N",
        "outputId": "1a6d89f6-356e-460c-af23-5c8e9b107770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-31 08:49:59--  https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.11, 3.165.160.59, 3.165.160.12, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://us.gcp.cdn.hf.co/xet-bridge-us/645e8da96320b0efe40ade7a/e2a1497efc1aa51b2da2a849d5dd2cd153d5bc024901afeade7e35379d8f7b52?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories-train.txt%3B+filename%3D%22TinyStories-train.txt%22%3B&response-content-type=text%2Fplain&Expires=1769852999&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiRXBvY2hUaW1lIjoxNzY5ODUyOTk5fX0sIlJlc291cmNlIjoiaHR0cHM6Ly91cy5nY3AuY2RuLmhmLmNvL3hldC1icmlkZ2UtdXMvNjQ1ZThkYTk2MzIwYjBlZmU0MGFkZTdhL2UyYTE0OTdlZmMxYWE1MWIyZGEyYTg0OWQ1ZGQyY2QxNTNkNWJjMDI0OTAxYWZlYWRlN2UzNTM3OWQ4ZjdiNTJcXD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=RgCimAFhEWoS0NePNg6iYPnQAx1vxUG59i66dhjRMAfVIU66g7T6DNwZJL4JT6T5r-9Jjri9fLuTj-Z%7ExkknDWH%7ERF-XF%7E%7Elrd6VRFAZtAmjVs4bYoCMARVRXcVQhbsMA9L0X%7Ec-HukDeXtZZItY-DlFonRrMYEqHvedu7sOugyIUcA6PLdQO90k1ya5uj0F6eWTjROM3G0H86C8aFnLjSIvwcHx5Fg5Xj4RHEw0OaX3ZJw1%7E9Ak6eQueLJIcXaWsnYvkt8johQQ-%7E06Z2XplLKEZWpl37zfMCTCPHjunx-HHzWvLO1MDLSLq%7El-YaCot9A5qJSqEe7WjRWccx-upA__&Key-Pair-Id=KJLH8B0YWU4Y8M [following]\n",
            "--2026-01-31 08:49:59--  https://us.gcp.cdn.hf.co/xet-bridge-us/645e8da96320b0efe40ade7a/e2a1497efc1aa51b2da2a849d5dd2cd153d5bc024901afeade7e35379d8f7b52?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories-train.txt%3B+filename%3D%22TinyStories-train.txt%22%3B&response-content-type=text%2Fplain&Expires=1769852999&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiRXBvY2hUaW1lIjoxNzY5ODUyOTk5fX0sIlJlc291cmNlIjoiaHR0cHM6Ly91cy5nY3AuY2RuLmhmLmNvL3hldC1icmlkZ2UtdXMvNjQ1ZThkYTk2MzIwYjBlZmU0MGFkZTdhL2UyYTE0OTdlZmMxYWE1MWIyZGEyYTg0OWQ1ZGQyY2QxNTNkNWJjMDI0OTAxYWZlYWRlN2UzNTM3OWQ4ZjdiNTJcXD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=RgCimAFhEWoS0NePNg6iYPnQAx1vxUG59i66dhjRMAfVIU66g7T6DNwZJL4JT6T5r-9Jjri9fLuTj-Z%7ExkknDWH%7ERF-XF%7E%7Elrd6VRFAZtAmjVs4bYoCMARVRXcVQhbsMA9L0X%7Ec-HukDeXtZZItY-DlFonRrMYEqHvedu7sOugyIUcA6PLdQO90k1ya5uj0F6eWTjROM3G0H86C8aFnLjSIvwcHx5Fg5Xj4RHEw0OaX3ZJw1%7E9Ak6eQueLJIcXaWsnYvkt8johQQ-%7E06Z2XplLKEZWpl37zfMCTCPHjunx-HHzWvLO1MDLSLq%7El-YaCot9A5qJSqEe7WjRWccx-upA__&Key-Pair-Id=KJLH8B0YWU4Y8M\n",
            "Resolving us.gcp.cdn.hf.co (us.gcp.cdn.hf.co)... 34.120.165.110\n",
            "Connecting to us.gcp.cdn.hf.co (us.gcp.cdn.hf.co)|34.120.165.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1924281556 (1.8G) [text/plain]\n",
            "Saving to: â€˜TinyStories-train.txtâ€™\n",
            "\n",
            "TinyStories-train.t 100%[===================>]   1.79G  3.03MB/s    in 6m 5s   \n",
            "\n",
            "2026-01-31 08:56:04 (5.03 MB/s) - â€˜TinyStories-train.txtâ€™ saved [1924281556/1924281556]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true -O TinyStories-train.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download & Process the Data for JAX\n",
        "import os\n",
        "import json\n",
        "import zipfile\n",
        "# 1. Download the GuoFeng Corpus directly\n",
        "# We use the 'dl=1' flag to get the file directly\n",
        "print(\"â¬‡ï¸ Downloading GuoFeng Webnovel Corpus...\")\n",
        "!wget -O webnovel_data.zip \"https://www.dropbox.com/scl/fo/dtrf3pe1vfbo5nse16648/AAZ5SFnuwohj7IJ2J-Q8zHs?rlkey=486vbn17qra1ez91btj0n4xu2&e=1&dl=1\"\n",
        "# 2. Unzip\n",
        "print(\"ğŸ“‚ Unzipping...\")\n",
        "with zipfile.ZipFile(\"webnovel_data.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"webnovel_raw\")\n",
        "# 3. Convert to JSONL (JAX friendly format)\n",
        "# We will combine train.zh (Chinese) and train.en (English)\n",
        "# into a format where the model learns to translate or associate pairs.\n",
        "final_output_file = \"/content/webnovel_train.jsonl\"\n",
        "source_zh = \"webnovel_raw/V1/TRAIN/train.zh\"\n",
        "source_en = \"webnovel_raw/V1/TRAIN/train.en\"\n",
        "# Verify path logic (in case unzip structure varies)\n",
        "# Sometimes dropbox zips create a top-level folder. We search for the files.\n",
        "found_zh = None\n",
        "found_en = None\n",
        "for root, dirs, files in os.walk(\"webnovel_raw\"):\n",
        "    if \"train.zh\" in files: found_zh = os.path.join(root, \"train.zh\")\n",
        "    if \"train.en\" in files: found_en = os.path.join(root, \"train.en\")\n",
        "if found_zh and found_en:\n",
        "    print(f\"âœ… Found source files:\\n  ZH: {found_zh}\\n  EN: {found_en}\")\n",
        "\n",
        "    print(\"ğŸ”„ converting to JSONL...\")\n",
        "    with open(found_zh, 'r', encoding='utf-8') as f_zh, \\\n",
        "         open(found_en, 'r', encoding='utf-8') as f_en, \\\n",
        "         open(final_output_file, 'w', encoding='utf-8') as f_out:\n",
        "\n",
        "        count = 0\n",
        "        for line_zh, line_en in zip(f_zh, f_en):\n",
        "            zh_text = line_zh.strip()\n",
        "            en_text = line_en.strip()\n",
        "            if not zh_text or not en_text: continue\n",
        "\n",
        "            # --- CHOOSE YOUR TRAINING FORMAT HERE ---\n",
        "\n",
        "            # OPTION A: Translation (Instruction Tuning style)\n",
        "            # Use this if your model supports 'instruction' fields\n",
        "            # record = {\n",
        "            #     \"instruction\": \"Translate chinese to english\",\n",
        "            #     \"input\": zh_text,\n",
        "            #     \"output\": en_text\n",
        "            # }\n",
        "            # OPTION B: Raw Text (Pretraining style)\n",
        "            # Use this if you are doing standard Causal Language Modeling\n",
        "            # The model just sees the Chinese followed by English\n",
        "            record = {\n",
        "                \"text\": f\"{zh_text}\\n{en_text}\"\n",
        "            }\n",
        "\n",
        "            f_out.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
        "            count += 1\n",
        "\n",
        "    print(f\"ğŸ‰ Done! Created {final_output_file} with {count} examples.\")\n",
        "else:\n",
        "    print(\"âŒ Could not find train.zh or train.en inside the zip.\")"
      ],
      "metadata": {
        "id": "2xkJpCFr-MZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKE2uUafLobI"
      },
      "source": [
        "Import the necessary modules, including JAX NumPy, Flax NNX, Optax, Grain, pandas, and Tiktoken:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ä¸€æ¬¡æ€§å®‰è£…æ‰€æœ‰å¯èƒ½ç¼ºå¤±çš„åº“\n",
        "!pip install tiktoken grain-nightly flax optax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnoAQLrwSCo3",
        "outputId": "c4117a5f-577f-48ee-8674-c4001f9c16b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: grain-nightly in /usr/local/lib/python3.12/dist-packages (0.2.16.dev20260112)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.12/dist-packages (0.11.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (0.2.6)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2026.1.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from grain-nightly) (1.4.0)\n",
            "Requirement already satisfied: array-record>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from grain-nightly) (0.8.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from grain-nightly) (3.1.2)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from grain-nightly) (1.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from grain-nightly) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=5.28.3 in /usr/local/lib/python3.12/dist-packages (from grain-nightly) (6.33.4)\n",
            "Requirement already satisfied: jax>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from flax) (0.8.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax) (1.1.2)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax) (0.11.32)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax) (0.1.80)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax) (14.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax) (4.15.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax) (6.0.3)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax) (0.1.10)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax) (0.1.90)\n",
            "Requirement already satisfied: jaxlib>=0.5.3 in /usr/local/lib/python3.12/dist-packages (from optax) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (75.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax) (1.1.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.0->flax) (0.5.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.0->flax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax>=0.6.0->flax) (1.16.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax) (2.19.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->grain-nightly) (2025.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->grain-nightly) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->grain-nightly) (3.23.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (25.1.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (4.15.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (3.20.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax) (5.9.5)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MKYFNOhdLq98"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding # For data and model parallelism (explained in more detail later)\n",
        "from jax.experimental import mesh_utils\n",
        "\n",
        "import flax.nnx as nnx\n",
        "import optax\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import grain.python as pygrain\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyt7MV6prz1"
      },
      "source": [
        "## Define the miniGPT model with Flax and JAX automatic parallelism\n",
        "\n",
        "### Leveraging JAX's data and tensor parallelism\n",
        "\n",
        "One of the most powerful features of JAX is [device parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for SPMD.\n",
        "\n",
        "- The data parallelism technique enables, for example, the training data to run via multiple parts (this is called sharding) - batches - in parallel and simultaneously across different devices, such as GPUs and Google TPUs. This allows to use larger batch sizes to speed up training.\n",
        "- Tensor parallelism allows us to split the model parameter tensors across several devices (sharding model tensors).\n",
        "- You can learn more about the basics of JAX parallelism in more detail in the [Introduction to parallel programming](https://jax.readthedocs.io/en/latest/sharded-computation.html) on the JAX documentation site.\n",
        "\n",
        "In this example, we'll utilize a 4-way data parallel and 2-way tensor parallel setup, which is aligned with Kaggle TPU v5e-8 or newer GCP TPUs chips.\n",
        "\n",
        "Note that as of October 2025, free-tier Colab only offers TPU v5e-1, which can no longer support SPMD.\n",
        "\n",
        "### jax.sharding.Mesh\n",
        "\n",
        "Earlier, we imported [`jax.sharding.Mesh`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh) - is a multidimensional NumPy array of JAX devices, where each axis of the mesh has a name, such as `'x'` or `'y'`. This will help encapsulate the information about the TPU resource organization for distributing computations across the devices.\n",
        "\n",
        "Our `Mesh` will have two arguments:\n",
        "- `devices`: This will take the value of [`jax.experimental.mesh_utils((4, 2))`](https://jax.readthedocs.io/en/latest/jax.experimental.mesh_utils.html), enabling us to build a device mesh. It is a NumPy ndarray with JAX devices (a list of devices from the JAX backend as obtained from [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html#jax.devices))..\n",
        "- `axis_names`, where:\n",
        "  - `batch`: 4 devices along the first axis - i.e. sharded into 4 - for data parallelism; and\n",
        "  - `model`: 2 devices along the second axis - i.e. sharded into 2 -  for tensor parallism\n",
        "\n",
        "This matches the structure in the Kaggle TPU v5e setup.\n",
        "\n",
        "Let's instantiate `Mesh` as `mesh` and declare the TPU configuration to define how data and model parameters are distributed across the devices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xuMlCK3Q8WJD"
      },
      "outputs": [],
      "source": [
        "# Create a `Mesh` object representing TPU device arrangement.\n",
        "# For example, for Kaggle TPU v5e-8:\n",
        "if jax.device_count() == 8:\n",
        "    mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
        "\n",
        "    ### Alternatively, we could use the 8-way data parallelism with only one line of code change.\n",
        "    ### JAX enables quick experimentation with different partitioning strategies\n",
        "    ### like this. We will come back to this point at the end of this tutorial.\n",
        "    # mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))\n",
        "\n",
        "### For free-tier Colab TPU, which only has a single TPU core\n",
        "if jax.device_count() == 1:\n",
        "    mesh = Mesh(mesh_utils.create_device_mesh((1, 1)), (\"batch\", \"model\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZKdhNo98NgG"
      },
      "source": [
        "We will use the GPT-2 tokenizer from the [Tiktoken](https://github.com/openai/tiktoken) library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iWbkk1V7-Isg"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XHQ0BQ9-KIj"
      },
      "source": [
        "To leverage model parallelism, we need to instruct the JAX compiler how to shard the model tensors across the TPU devices. Earlier, we also imported [`jax.sharding.PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) and [`jax.sharding.NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding):\n",
        "- [`PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) (using alias `P`) defines how tensors are sharded across the devices in our `Mesh`. Its elements describe how an input dimension is partitioned across mesh dimensions. For example, in `PartitionSpec('x', 'y')` the first dimension of data is sharded across `x` axis of the mesh, and the second one - across the `y` axis.\n",
        "  - We'll use `PartitionSpec` to describe how to shard a tensor across, for example, the `model` axis or be replicated on other dimensions (which is denoted by `None`).\n",
        "- [`NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding) is a (`Mesh`, `PartitionSpec`) pair that describes how to shard a model tensor across our `mesh`.\n",
        "- We combine `Mesh` (the TPU resources) with `PartitionSpec` and create a `NamedSharding`, which instructs how to shard each model tensor across the TPU devices.\n",
        "\n",
        "Additionally, we'll use Flax NNX's [`flax.nnx.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/spmd.html#flax.nnx.with_partitioning) to let each model layer know that the model weights or tensors need to be sharded according to our specification. We need to do this for every tensor/layer in the model.\n",
        "- `nnx.with_partitioning` will take two arguments, such as the `initializer` (such as [`flax.nnx.initializers.xavier_uniform`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.xavier_uniform) and [`flax.nnx.initializers.zeros_init`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.zeros_init)) and `sharding` (e.g. `NamedSharding(Mesh, PartitionSpec)` or `NamedSharding(mesh, P('model')` in our case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z0p-IHurrB9i"
      },
      "outputs": [],
      "source": [
        "# Define a triangular mask for causal attention with `jax.numpy.tril` and `jax.numpy.ones`.\n",
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    \"\"\" A single Transformer block.\n",
        "\n",
        "    Each Transformer block processes input sequences via self-attention and feed-forward networks.\n",
        "\n",
        "    Args:\n",
        "        embed_dim (int): Embedding dimensionality.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        ff_dim (int): Dimensionality of the feed-forward network.\n",
        "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
        "        rate (float): Dropout rate. Defaults to 0.1.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
        "        # Multi-Head Attention (MHA) with `flax.nnx.MultiHeadAttention`.\n",
        "        # Specifies tensor sharding (depending on the mesh configuration)\n",
        "        # where we shard the weights across devices for parallel computation.\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
        "                                          in_features=embed_dim,\n",
        "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                          rngs=rngs)\n",
        "        # The first dropout with `flax.nnx.Dropout`.\n",
        "        self.dropout1 = nnx.Dropout(rate=rate)\n",
        "        # First layer normalization with `flax.nnx.LayerNorm`.\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         rngs=rngs)\n",
        "        # The first linear transformation for the feed-forward network with `flax.nnx.Linear`.\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
        "                                  out_features=ff_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  rngs=rngs)\n",
        "        # The second linear transformation for the feed-forward network with `flax.nnx.Linear`.\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
        "                                  out_features=embed_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  rngs=rngs)\n",
        "        # The second dropout with `flax.nnx.Dropout`.\n",
        "        self.dropout2 = nnx.Dropout(rate=rate)\n",
        "        # Second layer normalization with `flax.nnx.LayerNorm`.\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                         rngs=rngs)\n",
        "\n",
        "\n",
        "    # Apply the Transformer block to the input sequence.\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        _, seq_len, _ = input_shape\n",
        "\n",
        "        # Instantiate the causal attention mask.\n",
        "        mask = causal_attention_mask(seq_len)\n",
        "\n",
        "        # Apply Multi-Head Attention with the causal attention mask.\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=inputs,\n",
        "            mask=mask,\n",
        "            decode=False\n",
        "        )\n",
        "        # Apply the first dropout.\n",
        "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
        "        # Apply the first layer normalization.\n",
        "        out1 = self.layer_norm1(inputs + attention_output)\n",
        "\n",
        "        # The feed-forward network.\n",
        "        # Apply the first linear transformation.\n",
        "        ffn_output = self.linear1(out1)\n",
        "        # Apply the ReLU activation with `flax.nnx.relu`.\n",
        "        ffn_output = nnx.relu(ffn_output)\n",
        "        # Apply the second linear transformation.\n",
        "        ffn_output = self.linear2(ffn_output)\n",
        "        # Apply the second dropout.\n",
        "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
        "        # Apply the second layer normalization and return the output of the Transformer block.\n",
        "        return self.layer_norm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "    \"\"\" Combines token embeddings (words in an input sentence) with\n",
        "    positional embeddings (the position of each word in a sentence).\n",
        "\n",
        "    Args:\n",
        "        maxlen (int): Matimum sequence length.\n",
        "        vocal_size (int): Vocabulary size.\n",
        "        embed_dim (int): Embedding dimensionality.\n",
        "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
        "    \"\"\"\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
        "        # Initialize token embeddings (using `flax.nnx.Embed`).\n",
        "        # Each unique word has an embedding vector.\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
        "        # Initialize positional embeddings (using `flax.nnx.Embed`).\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
        "\n",
        "    # Takes a token sequence (integers) and returns the combined token and positional embeddings.\n",
        "    def __call__(self, x):\n",
        "        # Generate a sequence of positions for the input tokens.\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        # Look up the positional embeddings for each position in the input sequence.\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        # Look up the token embeddings for each token in the input sequence.\n",
        "        token_embedding = self.token_emb(x)\n",
        "        # Combine token and positional embeddings.\n",
        "        return token_embedding + position_embedding\n",
        "\n",
        "class MiniGPT(nnx.Module):\n",
        "    \"\"\" A miniGPT transformer model, inherits from `flax.nnx.Module`.\n",
        "\n",
        "    Args:\n",
        "        maxlen (int): Maximum sequence length.\n",
        "        vocab_size (int): Vocabulary size.\n",
        "        embed_dim (int): Embedding dimensionality.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        feed_forward_dim (int): Dimensionality of the feed-forward network.\n",
        "        num_transformer_blocks (int): Number of transformer blocks. Each block contains attention and feed-forward networks.\n",
        "        rngs (nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
        "    \"\"\"\n",
        "    # Initialize miniGPT model components.\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        # Initiliaze the `TokenAndPositionEmbedding` that combines token and positional embeddings.\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        # Create a list of `TransformerBlock` instances.\n",
        "        # Each block processes input sequences using attention and feed-forward networks.\n",
        "        self.transformer_blocks = [TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
        "        ) for _ in range(num_transformer_blocks)]\n",
        "        # Initialize the output `flax.nnx.Linear` layer producing logits over the vocabulary for next-token prediction.\n",
        "        self.output_layer = nnx.Linear(in_features=embed_dim,\n",
        "                                       out_features=vocab_size,\n",
        "                                       kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                       bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                       rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        # Pass the input tokens through the `embedding_layer` to get token embeddings.\n",
        "        # Apply each transformer block sequentially to the embedded input, use the `training` flag for the behavior of `flax.nnx.Dropout`.\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "        # Pass the output of the transformer blocks through the output layer,\n",
        "        # and obtain logits for each token in the vocabulary (for next token prediction).\n",
        "        outputs = self.output_layer(x)\n",
        "        return outputs\n",
        "\n",
        "    @nnx.jit\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    @nnx.jit\n",
        "    def generate_step(self, padded_tokens, sample_index):\n",
        "        logits = self(padded_tokens)\n",
        "        next_token = self.sample_from(logits[0][sample_index])\n",
        "        return next_token\n",
        "\n",
        "    def generate_text(self, max_tokens, start_tokens):\n",
        "        generated = []\n",
        "        print(tokenizer.decode(start_tokens), flush=True, end='')\n",
        "        for i in range(max_tokens):\n",
        "            sample_index = len(start_tokens) + len(generated) - 1\n",
        "\n",
        "            padded_tokens = jnp.array((start_tokens + generated + [0] * (maxlen - len(start_tokens) - len(generated))))[None, :]\n",
        "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
        "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
        "              break\n",
        "            generated.append(next_token)\n",
        "            # decode and print next_token\n",
        "            print(tokenizer.decode([next_token]), flush=True, end='')\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "# Creates the miniGPT model with 4 transformer blocks.\n",
        "def create_model(rngs):\n",
        "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks=4, rngs=rngs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igX_eoGNMTGR"
      },
      "source": [
        "Set some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GRhiDsCrMZRp"
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "num_transformer_blocks = 8\n",
        "maxlen = 256\n",
        "embed_dim = 256\n",
        "num_heads = 8\n",
        "feed_forward_dim = 256\n",
        "batch_size = 144 * jax.device_count() / 2  # divide by 2 in case of model parallelism\n",
        "if jax.device_count() == 1:\n",
        "    batch_size = 144\n",
        "num_epochs = 1\n",
        "top_k = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI1ci-HyMspJ"
      },
      "source": [
        "## Loading and preprocessing the data\n",
        "\n",
        "Data loading and preprocessing with [Grain](https://github.com/google/grain)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rGUFsn1GMuzh"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TextDataset:\n",
        "    data: list\n",
        "    maxlen: int\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # Use Tiktoken for tokenization\n",
        "        encoding = tokenizer.encode(self.data[idx], allowed_special={'<|endoftext|>'})[:self.maxlen]  # Tokenize and truncate\n",
        "        return encoding + [0] * (self.maxlen - len(encoding))  # Pad to maxlen\n",
        "\n",
        "def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "      text = f.read()\n",
        "\n",
        "    stories = text.split('<|endoftext|>')\n",
        "    stories = [story+'<|endoftext|>' for story in stories if story.strip()]\n",
        "    df = pd.DataFrame({'text': stories})\n",
        "    data = df['text'].dropna().tolist()\n",
        "    dataset = TextDataset(data, maxlen)\n",
        "\n",
        "    sampler = pygrain.IndexSampler(\n",
        "        len(dataset),\n",
        "        shuffle=False,\n",
        "        seed=42,\n",
        "        shard_options=pygrain.NoSharding(),\n",
        "        num_epochs=num_epochs,\n",
        "    )\n",
        "\n",
        "    dl = pygrain.DataLoader(\n",
        "        data_source=dataset,\n",
        "        sampler=sampler,\n",
        "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
        "    )\n",
        "\n",
        "    return dl\n",
        "\n",
        "text_dl = load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKVSD8KSM1um"
      },
      "source": [
        "## Defining the loss function and training step function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8rRuTmABNV4b"
      },
      "outputs": [],
      "source": [
        "# Defines the loss function using `optax.softmax_cross_entropy_with_integer_labels`.\n",
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "# Define the training step with the `flax.nnx.jit` transformation decorator.\n",
        "@nnx.jit\n",
        "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(model, grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5um2vkeUNckm"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Start training. It takes ~50 minutes on Colab.\n",
        "\n",
        "Note that for data parallel, we are sharding the training data along the `batch` axis using `jax.device_put` with `NamedSharding`.\n",
        "\n",
        "We are also using the `jax.vmap` transformation to produce the target sequences faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysl6CsfENeJN",
        "outputId": "5624ab09-3c78-4732-873c-d5c20299df25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial generated text:\n",
            "Once upon a timeaciaGender gearuser Analysisval {} Bruce Lauren helic Lauren Bruce againstliterally SQU retire Path {}valascript northwest {} Bruceuit Pathascript northwestdrops freelyvic996 curated hysteria survivor {}sclaxteradvert Sitting qualifiers snack {} scenariovalameron {} Path {}Nick VeganExcept peasantascript Whites retire {} retire {} Analysisrest {} Mine psychedelic flankForgeModLoader Path Bravo {} inflic {} strutConnector psychedelic beyond Beforeocker interesting Dani {}sclaxter retire {}Nick sorrow Typesrest interestingUV FSyrus resorts {} Dani {} perished {} retire interesting sorrow reversibleurned {} Womanlast 118 reass gentlestudyManager {} retire {} verb Captain forbid Bruce {} Analysis ox {} inexplicable tumor psychedelic {} serverpel perished Tang {} cropDisclaimeruti nond {} scenario teach serverlast {} Woman {}absor northwestroid variable {} Whites {} dancers iPod {} {}valolate Assist hiding ox {}ampionscre lineman servesShould decision psychedelicShould beyondwaves {} retire interesting Tangresterv ribbon complicationsaggressiverest {} SessionSmith {}Nick abnorm dissatisfiedundrum {} perished {} Gustav rolled shamefulundrum retire {}valundrumlast {}val {} perished Brigham Analysis developerscre Atom {}scl HouthInteger {} northwest appease miles {} perished THR Hyundai Captainã‚± {} Cube psychedelic {} inflic {} retire {} Whites dancers {}scl FS lore appease Din {} Whites abnorm[] {} {}scl FS appease dangling Bruce abnormcre97 psychedeliccre!!!\n",
            "\n",
            "Step 200, Loss: 4.834714412689209, Elapsed Time: 54.03 seconds\n",
            "Generated text:\n",
            "Once upon a time a a a a a a a.\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".\n",
            ".!!!!\n",
            "\n",
            "Step 400, Loss: 3.774630546569824, Elapsed Time: 84.35 seconds\n",
            "Generated text:\n",
            "Once upon a time there was time, there was a little girl. One day to very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very!!!!\n",
            "\n",
            "Step 600, Loss: 3.3864734172821045, Elapsed Time: 17.29 seconds\n",
            "Generated text:\n",
            "Once upon a time there was a little girl named Lily. She loved to play. One day, years friends. One day, she had a big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big park.\n",
            "One day, she was very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very always wanted to go.\n",
            "One, she was very very very very very very so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so so a little girl was go.\n",
            "The girl.\n",
            "The girl was so so so so so so so so so the little girl was so so so so so so so so so so so so so so so so so so so so so so so so!!!!\n",
            "\n",
            "Step 800, Loss: 3.188941240310669, Elapsed Time: 16.92 seconds\n",
            "Generated text:\n",
            "Once upon a time there was a little girl. She loved to play. One day, years friends. One day, years old and saw. One day, she saw. One day, she wanted to the big and saw. He saw. He saw. He saw a big big big big and saw. He saw a big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big big.\n",
            "\"'t the little big big.\n",
            "\"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"I said, \"!!!!\n",
            "\n",
            "Step 1000, Loss: 3.0732345581054688, Elapsed Time: 17.04 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside. One day, Lily's mommymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymy her mommymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymymy her mom her mom that.\n",
            "\"I's okay. I can have youilymymymymymymymymymymymymymymymymymymymymymy!!!!\n",
            "\n",
            "Step 1200, Loss: 2.863478422164917, Elapsed Time: 17.23 seconds\n",
            "Generated text:\n",
            "Once upon a time a little girl named Lily. Tim loved to play with his toys. One day, they would play with his toys. One day, they would go to the park.\n",
            "One day, they saw a big, he was a big, he was a big, he was a big, he was a big, but he was very happy. He was very sad and wanted to the tree.\n",
            "Lily was so happy. He was so happy and wanted to the bird. He was so happy. He was so happy and he could not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not!!!!\n",
            "\n",
            "Step 1400, Loss: 2.5589334964752197, Elapsed Time: 16.95 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys, but she would go to the park. One day, she saw a big, shiny flower in the ground. She was very happy. She wanted to play with her mommy, but she was too small.\n",
            "She saw a big, and a big, she wanted to play with her mommy, but she was very sad. She asked her mommy, \"What is a good idea?\"\n",
            "Her mommy said, \"I'm so pretty. She said, \"I'm sorry, but you can be careful. You can be careful with you.\"\n",
            "Lily was sad. She said, \"I'm sorry, but you want to be careful. You can be careful with me.\"\n",
            "Lily was sad and said, \"I'm sorry for you. You're a good time to play with me.\"\n",
            "\n",
            "\n",
            "Step 1600, Loss: 2.361205816268921, Elapsed Time: 16.50 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys. One day, she went to the park. She saw a big, red cat. She wanted to play with it. She asked her mom, \"Can I help you?\"\n",
            "Lily said, \"Yes, Lily. It's a little girl. It's a little girl. She wanted to play with it. She said, \"No, Lily, Lily, Lily, Lily, Lily, Lily, Lily, Lily, Lily, but you can't not find it. She took the ball and the ball.\n",
            "Lily was happy and played with her friends. She played with her toys and played with her friends. She played with her toys and played with her friends. Lily was happy. She played with her toys and played with her toys.\n",
            "\n",
            "\n",
            "Step 1800, Loss: 2.205888271331787, Elapsed Time: 16.58 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and toys. One day, she went to the park. She saw a big slide and wanted to play. She asked her mommy if she could have the ball.\n",
            "Lily said, \"No, Lily, I want to play with you.\" Lily said, \"No, I have a lot, I want to play with you.\"\n",
            "Lily was sad and said, \"Okay, I want to play with you.\"\n",
            "Lily and her mommy played with the ball and the ball. They played with the ball and the ball. They played with the ball and the ball. They played with the ball and the ball.\n",
            "\n",
            "\n",
            "Step 2000, Loss: 2.088730812072754, Elapsed Time: 16.42 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and books. One day, she found a big box in her backyard. She wanted to play with it. She asked her mom, \"Can I play with you?\" Her mom said, \"Yes, but you have to be careful. It's not to hurt yourself.\"\n",
            "Lily was sad and didn't know what to do. She asked her mom, \"Why don't you fix your toys?\" Her mom said, \"I don't know what's wrong.\" Her mom said, \"I don't want to be careful. I will be careful and not find your toys.\"\n",
            "Lily was sad and didn't want to play with her toys. She tried to make her mom's toys, but she was too hard. She tried to make her mom's toys, but she was too hard. She tried to make her mom's toys and she tried to make her clean up her toys. She tried to make her mom's toys, but it was too late. She tried to make her mom's toys and she was very sad. She learned that it's okay and not to make mistakes.\n",
            "\n",
            "\n",
            "Step 2200, Loss: 1.957518219947815, Elapsed Time: 17.13 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and always wanted to play with her toys. One day, Lily's mom asked her to help her clean up the room. Lily was so excited to see what was inside.\n",
            "But then, she saw a big box in the box. She wanted to play with the box and play with it. She asked her mom, \"What's wrong?\" Her mom said, \"I lost my toys.\"\n",
            "Her mom said, \"Don't worry, Lily. I'll help you fix it.\"\n",
            "Lily was sad and didn't want to be mad. She asked her mom, \"What's wrong?\" Her mom said, \"I lost my toys and we can fix it.\"\n",
            "Her mom smiled and said, \"That's okay, Lily. We can fix it together.\"\n",
            "So, Lily and her mom went to the living room and found a new toy box. They were so happy and hugged each other. They both learned that sharing and sharing is important to be kind and kind and kind and kind and kind.\n",
            "\n",
            "\n",
            "Step 2400, Loss: 1.933328628540039, Elapsed Time: 16.88 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and make new friends. One day, Lily's mommy gave her a special present. Lily was so excited to have a special present. She wanted to show it to her friends.\n",
            "Later that day, Lily's mommy took her to the store. Lily was so excited to see the present. She asked her mommy to buy a present. Her mommy said they could buy it for her. Lily was so happy and she could have the present.\n",
            "\n",
            "\n",
            "Step 2600, Loss: 1.8254826068878174, Elapsed Time: 16.16 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her mommy. One day, she went to the park with her mommy and saw a big tree. She was so excited and wanted to pick it up.\n",
            "Lily's mommy said, \"Lily, it's too big for you to reach the tree.\" Lily was so excited. She ran to the tree and climbed up the tree. She climbed up the tree and climbed the tree.\n",
            "Lily's mommy said, \"Lily, you can't reach the tree. It's too high.\" Lily smiled and said, \"I can't reach the tree. I will get you.\" Her mommy smiled and said, \"Yes, Lily. I will be happy.\" Lily smiled and said, \"Thank you, Lily. You are a good friend.\"\n",
            "\n",
            "\n",
            "Step 2800, Loss: 1.80697762966156, Elapsed Time: 16.64 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her toys. One day, Lily's mommy gave her a big box of toys. Lily was so excited to see what she had found.\n",
            "\"Mommy, can I have a toy car?\" asked Lily.\n",
            "\"Sure, sweetie. Let's go to the store and buy a new toy car,\" replied her mommy.\n",
            "Lily was so excited to see the new toy car. She wanted to buy it, but it was too heavy.\n",
            "\"Mommy, can I have it?\" she asked.\n",
            "\"No, Lily, it's mine,\" her mommy said.\n",
            "Lily was sad and didn't know what to do. She asked her mommy, \"What do you want?\"\n",
            "Her mommy smiled and said, \"I want to buy it for you to buy it.\"\n",
            "Lily was so happy and said, \"I want to buy it. I want to buy it.\"\n",
            "\n",
            "\n",
            "Step 3000, Loss: 1.7728685140609741, Elapsed Time: 16.62 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and make her favorite toy. One day, Lily's mommy told her that she was going to make a special box of paper. Lily was so excited!\n",
            "When she got to the box, she saw a big box of paper with a picture of a picture of a rainbow. She was so excited and excited to see what was inside. She wanted to make a picture of a rainbow, so she decided to make a picture of a rainbow.\n",
            "When she got to the picture, she was so excited! She ran to the picture and showed it to her mommy. Her mommy was so proud of her and said, \"Wow, Lily! You are so talented!\" Lily was so happy and proud of herself.\n",
            "\n",
            "\n",
            "Step 3200, Loss: 1.7340518236160278, Elapsed Time: 16.37 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she found a big box in the garage. She was so excited and wanted to open it.\n",
            "As she was walking, she saw a big box. She was so excited and wanted to open it. She ran to the box and opened it. Inside the box, there was a big box. Inside the box was a box. Inside the box, there were lots of toys inside.\n",
            "Lily was so excited! She couldn't wait to open the box. She was so excited! She ran to the box and opened it. Inside the box, there was a big box of toys. Inside the box was a box full of toys.\n",
            "Lily was so happy and excited. She had found a new toy. She was so happy and excited. From that day on, Lily always remembered to open the box and take things.\n",
            "\n",
            "\n",
            "Step 3400, Loss: 1.7037931680679321, Elapsed Time: 16.76 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her favorite was a big, red ball. One day, Lily's mommy told her that she was going to be a good place to play. Lily was sad because she didn't want to play with her ball.\n",
            "Lily's mommy told her that she had to be careful and not to break the ball. Lily didn't want to play with her ball, but she didn't want to play with it. She tried and tried, but she couldn't. She tried and tried, but she couldn't. She tried and tried, but she couldn't. She tried and tried, but she couldn't. She was stuck and tried, but she could not find her way.\n",
            "Lily's mommy tried and tried, but she couldn't. She tried and tried, but she couldn't. She tried and tried, but she couldn't. Finally, her mommy came and saw the broken ball. She was very upset and sad. Lily's mommy told her that she had to fix her broken pieces. She said she could fix her broken pieces and fix her broken pieces.\n",
            "Lily learned that helping others can't fix her broken car!!!!\n",
            "\n",
            "Step 3600, Loss: 1.6924837827682495, Elapsed Time: 17.20 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she found a big, red ball in the grass. She was so happy and wanted to show it to her friends.\n",
            "Lily was so excited to show her friends. She wanted to show her friends her friends. She asked her friends to play with her. Her friends said yes, and they all went to the park. Lily was so happy.\n",
            "At the park, Lily saw a big tree. She wanted to climb it. She climbed up the tree and climbed the tree. She climbed up the tree and climbed the tree. She was so happy. She thanked her friends for helping her.\n",
            "\n",
            "\n",
            "Step 3800, Loss: 1.6409718990325928, Elapsed Time: 16.25 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she found a big, red ball in the park. She was so excited to show it to her friends.\n",
            "As she was playing, she saw a big, red ball. She wanted to play with it, but she didn't know how. She asked her mom, \"Can I play with the ball?\" Her mom said, \"Yes, but be careful. You can't play with it.\"\n",
            "Lily was sad, but she didn't know what to do. She asked her mom, \"Can I play with the ball?\" Her mom said, \"Yes, but be careful. Let's play together.\" So, they played together all day.\n",
            "\n",
            "\n",
            "Step 4000, Loss: 1.628416657447815, Elapsed Time: 16.34 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sun. One day, she found a big, shiny rock. She was so happy and excited.\n",
            "She ran to the rock and tried to pull it out. But it was too heavy for her. She tried and tried, but it wouldn't budge. She tried and tried, but it wouldn't budge.\n",
            "Then, her mom came into the room and saw the rock. She was so sad and angry. She told her mom that it was not safe and that it was just a bad ending.\n",
            "\n",
            "\n",
            "Step 4200, Loss: 1.6080652475357056, Elapsed Time: 16.09 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and explore the world. One day, she went to the park with her mommy and saw a big, red ball. She wanted to play with it, but her mommy said no.\n",
            "Lily was sad and didn't know what to do. She asked her mommy, \"Why are you sad?\" Her mommy said, \"I don't know, but I want to play with you.\"\n",
            "Lily thought about it and decided to give the ball to her mommy. She said, \"Mommy, I want to play with the ball.\" Her mommy smiled and said, \"Okay, but be careful. But you have to be careful and don't want to be careful.\"\n",
            "Lily felt sad and went to bed without her mommy. She was happy and said, \"I'm sorry, mommy. I didn't mean to break my ball.\" Her mommy smiled and said, \"It's okay, Lily. We can play together again tomorrow.\"\n",
            "\n",
            "\n",
            "Step 4400, Loss: 1.5982897281646729, Elapsed Time: 17.03 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She was very excited because she was going to visit her grandma. Her grandma was very excited and asked her to help her. Lily was so excited!\n",
            "When they arrived, Lily was so excited! She couldn't wait to see what was inside. She asked her grandma, \"Grandma, can I have some cookies?\"\n",
            "Grandma smiled and said, \"Yes, you can have some cookies. But first, we can't wait for them to eat.\"\n",
            "Lily was so excited and said, \"Yes, please!\"\n",
            "Grandma said, \"Yes, we can eat them.\" Lily was so happy and said, \"Thank you, Grandma. I love you!\"\n",
            "Grandma smiled and said, \"You're welcome, Lily. I love you.\"\n",
            "Lily smiled and said, \"Thank you, Grandma. I love you so much!\"\n",
            "\n",
            "\n",
            "Step 4600, Loss: 1.6153264045715332, Elapsed Time: 16.50 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She was very happy because she had a big box of toys. One day, Lily's mommy told her it was time to go to the park. Lily was excited to see the park and she wanted to play with her toys.\n",
            "Lily's mommy said, \"Lily, it's time to go to the park. We can play with the toys and play with the toys.\" Lily was so happy and said, \"Yes, I want to play with the toys.\"\n",
            "So, Lily and her mommy played with the toys and had lots of fun. They played and had lots of fun. When it was time to go home, Lily said, \"I want to play with the toys!\"\n",
            "\n",
            "\n",
            "Step 4800, Loss: 1.5967295169830322, Elapsed Time: 16.46 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little boy named Timmy. Timmy loved to play with his toy cars. One day, Timmy's mommy asked him to help her with the toy car. Timmy was very excited and asked his mommy if she could play with the car.\n",
            "His mommy said yes and they went to the store. Timmy was so happy and couldn't wait to play with his toy cars.\n",
            "At the store, Timmy's mommy saw that Timmy was sad and asked him what was wrong. Timmy said, \"I'm sorry, I didn't mean to make you sad.\" His mommy smiled and said, \"Don't worry, Timmy. I'll help you find your toy car.\"\n",
            "Timmy was so happy and thanked his mommy. He went home and told his mommy that he was going to play with his toy car. Timmy was so happy and thanked his mommy for helping him.\n",
            "\n",
            "\n",
            "Step 5000, Loss: 1.5650321245193481, Elapsed Time: 16.78 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she found a big, shiny rock in the grass. She was so excited to show it to her mom.\n",
            "\"Mommy, look what I found!\" she said.\n",
            "\"That's a great idea, Lily. I can show you a special rock to me. It's a special rock that you can use it to make it look like a special rock,\" her mom said.\n",
            "Lily was so happy to show her mom her new rock. She showed it to her mom and showed it to her mom. Her mom was so proud of her and showed her how to use the rock.\n",
            "\"Wow, that's a great rock!\" Lily said.\n",
            "\"Thank you, mommy. I love it!\" said her mom.\n",
            "Lily was so happy that she had such a special rock. She knew that she could always use the rock to make her happy.\n",
            "\n",
            "\n",
            "Step 5200, Loss: 1.5858328342437744, Elapsed Time: 16.92 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she found a big, shiny rock in the grass. She picked it up and showed it to her mom.\n",
            "\"Look, Mommy! A rock!\" Lily said.\n",
            "\"Wow, Lily, that's a rock!\" her mom said. \"It's a rock rock rock rock rock!\"\n",
            "Lily was so happy and started to rock the rock. She wanted to keep it forever.\n",
            "\"Can I keep it?\" she asked.\n",
            "\"Sure, Lily,\" her mom said. \"But you have to be careful when playing outside.\"\n",
            "Lily and her mom played together for a while, but they were not happy. They played together and had a great time.\n",
            "\n",
            "\n",
            "Step 5400, Loss: 1.5344213247299194, Elapsed Time: 16.36 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she found a big, shiny rock in the dirt. She picked it up and showed it to her mom.\n",
            "\"Mommy, what is that?\" asked Lily.\n",
            "\"It's a rock, sweetie. It's a rock,\" replied her mom.\n",
            "Lily was so excited. She wanted to keep the rock, so she started to rock it.\n",
            "\"Mommy, can you keep it?\" asked Lily.\n",
            "\"Sure, sweetie. But be careful, it's too heavy,\" said her mom.\n",
            "Lily was sad, but she knew she had to keep the rock.\n",
            "\"I'm sorry, mommy. I didn't mean to break it,\" said her mom.\n",
            "Lily felt bad, but she knew she had to be careful.\n",
            "\n",
            "\n",
            "Step 5600, Loss: 1.5150234699249268, Elapsed Time: 16.66 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore. One day, she saw a big, shiny thing in the sky. It was a big, shiny thing.\n",
            "Lily wanted to see what it was. She asked her mom, \"Can I touch it?\" Her mom said, \"No, it's not safe.\"\n",
            "Lily was sad and didn't know what to do. She asked her mom, \"Can I touch it?\" Her mom said, \"No, it's not safe.\"\n",
            "Lily felt bad and said, \"I don't like it. It's not safe.\" Her mom said, \"It's okay, but you can still touch it. It's not safe.\"\n",
            "\n",
            "\n",
            "Step 5800, Loss: 1.5453325510025024, Elapsed Time: 16.57 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sun. One day, she saw a big, red ball. She wanted to play with it, but it was too high for her to reach.\n",
            "Lily's mom saw her struggling and asked her what was wrong. Lily told her mom about the ball and how she was trying to reach it. Her mom said, \"Don't worry, Lily. I can help you get the ball.\"\n",
            "Lily was so happy and thanked her mom. She went to her mom and said, \"Thank you, mommy! I'm so proud of you.\" Her mom smiled and said, \"You're welcome, Lily. I'm glad you're here.\"\n",
            "\n",
            "\n",
            "Step 6000, Loss: 1.5208736658096313, Elapsed Time: 16.27 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside. One day, she saw a big, red balloon. It was red and shiny. She wanted to touch it, but she was too high.\n",
            "Lily's mommy came outside and saw the balloon. She said, \"Lily, it's too high for you to reach.\" Lily was so happy and said, \"Yes, I want to touch it!\"\n",
            "So, Lily and her mommy went to the balloon. They both had a lot of fun playing together. When it was time to go home, Lily said, \"Thank you for letting me touch it!\" Her mommy smiled and said, \"You're welcome, Lily. I'm glad you like it.\"\n",
            "\n",
            "\n",
            "Step 6200, Loss: 1.5446826219558716, Elapsed Time: 16.49 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her friends. One day, Lily's mommy gave her a special gift. Lily was so happy and she couldn't wait to show her friends.\n",
            "Lily's mommy told her that she was going to receive a gift from her grandma. Lily was so excited and couldn't wait to show her grandma. She was so excited and couldn't wait to show her grandma.\n",
            "When Lily's grandma arrived, she was so excited to see all the gifts she had seen. She was so happy and couldn't wait to show her grandma. Lily was so happy and excited to show her grandma her.\n",
            "\n",
            "\n",
            "Step 6400, Loss: 1.4934602975845337, Elapsed Time: 16.22 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little boy named Timmy. Timmy loved to play with his toys and his friends. One day, Timmy's mom told him that he was going to the park because he was going to be a good boy. Timmy was excited to go to the park and play with his friends.\n",
            "At the park, Timmy saw a big slide. He wanted to climb it and see what was on. Timmy's mom told him that he was going to climb the ladder. Timmy was so excited! He climbed the ladder and climbed the ladder. Timmy climbed up the ladder and climbed the ladder. He was so happy!\n",
            "When Timmy was done, he was so proud of himself. He was so proud of himself for being so brave and playing with his friends. Timmy was so happy that he had a great day at the park.\n",
            "\n",
            "\n",
            "Step 6600, Loss: 1.4951220750808716, Elapsed Time: 16.63 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little boy named Timmy. Timmy loved to play with his toys and his favorite toy was a big, red ball. One day, Timmy's mom asked him to help her make dinner. Timmy was so excited to help his mom.\n",
            "His mom said, \"Timmy, you need to be careful not to be careless. You need to be careful not to be careless.\" Timmy nodded and said, \"Okay, mom. I will be careful with my toys.\"\n",
            "Later that day, Timmy's mom came into the room and saw that Timmy was upset. She said, \"Timmy, you need to be careful with your toys. You need to be careful and not be careless.\" Timmy felt bad and said, \"I don't want to be careless. I'm sorry, mom.\"\n",
            "His mom said, \"Timmy, you can be careful with your toys. You can be careful and be careful with your toys.\" Timmy smiled and said, \"Okay, mom. I'll be careful with my toys.\"\n",
            "\n",
            "\n",
            "Step 6800, Loss: 1.484438419342041, Elapsed Time: 16.92 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her friends. One day, she went to the park to play. She saw a big, shiny rock and decided to take it home. She put the rock in her pocket and went to the park.\n",
            "When she got home, she showed her mom the rock and dad said, \"Wow, that's a nice rock! Can I keep it?\" Lily smiled and said, \"Yes, but remember to be careful.\"\n",
            "So, Lily went home and put the rock in her pocket. She was so happy to have a new friend. She was so proud of herself for finding the rock and keeping it safe.\n",
            "\n",
            "\n",
            "Step 7000, Loss: 1.4855058193206787, Elapsed Time: 16.59 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, red ball. She wanted to play with it, but it was too high.\n",
            "Lily's mommy came outside and saw the ball. She said, \"Lily, you need to be careful.\" Lily was sad because she didn't want to get hurt.\n",
            "Lily's mommy told her that she should be careful with the ball. She said, \"If you want to play with the ball, you can play with it.\" Lily was happy and played with the ball all day.\n",
            "\n",
            "\n",
            "Step 7200, Loss: 1.501179575920105, Elapsed Time: 16.15 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she went to the park with her mommy and saw a big tree. She wanted to climb it, but it was too high.\n",
            "Lily's mommy said, \"Let's go and see the tree and see if it might get stuck.\"\n",
            "Lily was excited and said, \"Yes, let's go!\"\n",
            "They went to the tree and climbed up the tree. They climbed up the tree and climbed up the tree. Lily was so happy and she could finally reach the top.\n",
            "But then, something bad happened. The tree started to shake and the tree fell down. Lily was scared and tried to help, but it was too late. The tree was too strong and the tree was stuck.\n",
            "Lily's mommy was very sad and said, \"I'm sorry, Lily. I know you can help you.\"\n",
            "Lily's mommy hugged her and said, \"I'm sorry, but you have to be careful. I know you can help you.\"\n",
            "\n",
            "\n",
            "Step 7400, Loss: 1.484629511833191, Elapsed Time: 16.94 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, red ball in the park. She wanted to play with it, but she didn't know how.\n",
            "Lily's mom saw her and said, \"Lily, you have to be careful with your ball. It's a big red ball.\" Lily was so happy and excited. She ran to her mom and said, \"Mommy, can I play with my ball?\" Her mom smiled and said, \"Yes, Lily. You can play with your ball and you can play with it.\"\n",
            "Lily played with her ball all day. She threw it and it bounced around the park. She laughed and laughed. Suddenly, she heard a noise and saw a big, scary dog. Lily was scared and ran to her mom. Her mom hugged her and said, \"Don't worry, Lily. We can play together again.\"\n",
            "\n",
            "\n",
            "Step 7600, Loss: 1.4446121454238892, Elapsed Time: 16.90 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her favorite toy was a teddy bear. One day, Lily's mommy asked her to help with the laundry. Lily was so excited to help her mommy with the laundry.\n",
            "As they were folding the laundry, Lily's mommy said, \"Lily, let's go to the laundry room and see the laundry!\" Lily was so happy and said, \"Yay! I want to play with the laundry!\"\n",
            "Her mommy said, \"That's a good idea, Lily. Let's go to the laundry room and see the laundry.\" Lily was so excited and said, \"Yay! I want to play with the laundry!\"\n",
            "Her mommy smiled and said, \"That's great, Lily. Let's go to the laundry room and see how pretty it is.\" Lily was so happy and said, \"Yay! I love the laundry!\"\n",
            "\n",
            "\n",
            "Step 7800, Loss: 1.465162754058838, Elapsed Time: 16.60 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and sing songs. One day, she went to the park with her mommy and saw a big tree. She wanted to climb the tree, but she was too scared.\n",
            "Lily's mommy came to her and said, \"Don't worry, Lily. I will help you climb the tree. I will stand and stand on the tree.\"\n",
            "Lily was so happy and said, \"Thank you, mommy! I love you!\"\n",
            "After playing, Lily and her mommy went home. She told her mommy about the tree and how she got the hang of it. Her mommy was so proud of her and said, \"You did a great job, Lily. You are a good girl.\"\n",
            "Lily smiled and said, \"Thank you, mommy. I love you too, mommy. I love you too, Lily. I love you too, even if you are scared.\"\n",
            "\n",
            "\n",
            "Step 8000, Loss: 1.4441914558410645, Elapsed Time: 16.62 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, red ball in the park. She wanted to play with it, but her mom said no.\n",
            "Lily was sad because she really wanted to play with the ball. She asked her mom if she could play with the ball. Her mom said yes, and they played together.\n",
            "After playing, Lily played with the ball all day. She played with the ball, and she was very happy. She played with the ball all day long.\n",
            "\n",
            "\n",
            "Step 8200, Loss: 1.4369159936904907, Elapsed Time: 16.06 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her friends. One day, Lily's mom said, \"Let's go to the store and buy some food for your family.\" \n",
            "Lily was so excited and said, \"Yay! I love food!\" \n",
            "When they got to the store, they saw a big, juicy steak. Lily said, \"Yum! I love steak!\" \n",
            "Her mom said, \"Let's go to the store and buy some steak for our family.\" \n",
            "Lily was so happy and said, \"Yay! I love steak!\" \n",
            "When they got to the store, they saw that the steak was very cheap. Lily said, \"Thank you, mommy! This steak is so yummy!\" \n",
            "Her mom smiled and said, \"You're welcome, sweetie. I love you very much.\" \n",
            "Lily was so happy to have a new friend and she couldn't wait to go to the store to buy some new steak for her family.\n",
            "\n",
            "\n",
            "Step 8400, Loss: 1.4584633111953735, Elapsed Time: 16.71 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her friends. One day, Lily's mom told her to go to the park and play with her toys. Lily was so excited to go to the park and play.\n",
            "At the park, Lily saw a big slide. She wanted to go on it. She asked her mom if she could go on the slide. Her mom said yes, but only if she was brave enough to go on the slide. Lily was so happy!\n",
            "As they were sliding down the slide, Lily saw a little boy who was crying. The little boy asked if he wanted to play too. Lily said yes and the little boy ran to the slide. They both laughed and played together. Lily was so happy to be on the slide.\n",
            "\n",
            "\n",
            "Step 8600, Loss: 1.4610767364501953, Elapsed Time: 16.41 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she found a shiny rock and decided to take it home.\n",
            "Lily was so excited to see the rock and wanted to keep it. She asked her mom, \"Can I keep it, please?\" Her mom said, \"No, Lily. You can't have it.\"\n",
            "Lily was sad, but she knew she had to be careful. She put the rock back and went to find the rock. She found it and put it back on the rock. She was so happy and proud of herself.\n",
            "The next day, Lily went back to the rock and saw the rock again. She was so happy and proud of herself. She knew that she could keep the rock safe and sound.\n",
            "\n",
            "\n",
            "Step 8800, Loss: 1.4432884454727173, Elapsed Time: 16.53 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she went for a walk in the park and saw a big tree. She wanted to climb it, but she couldn't reach it. \n",
            "Lily asked her mommy, \"Can I help you?\" Her mommy replied, \"Yes, but you must be careful.\" So, Lily took a deep breath and started to climb the tree. She climbed higher and higher until she reached the top. \n",
            "As she was climbing, she saw a beautiful view. She was so happy and excited to see the view. She ran and climbed until she reached the top. She looked down and saw the view was beautiful. She felt so happy and proud of herself. From that day on, Lily knew that she could always count on her mommy to help her.\n",
            "\n",
            "\n",
            "Step 9000, Loss: 1.4287519454956055, Elapsed Time: 16.77 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her friends. One day, Lily's mommy told her that she was going to have a special surprise for her. Lily was so excited to see what was inside the present.\n",
            "When they got to the present, Lily was so excited to see what was inside. She opened the present and inside was a big, shiny box. Inside, there were lots of toys and games. Lily was so happy to see all of her toys.\n",
            "But then, something bad happened. Lily's mommy called her to come and see what was inside. Lily was very sad. She didn't know what to do. She asked her mommy what was inside the box. Her mommy explained that it was a toy that was a toy that could make all of her friends. Lily was so happy that she hugged her mommy and thanked her mommy for being so thoughtful. From that day on, Lily always remembered to be thoughtful and kind to all of her friends.\n",
            "\n",
            "\n",
            "Step 9200, Loss: 1.4281278848648071, Elapsed Time: 16.76 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she went for a walk and saw a big, shiny rock. She was so excited and wanted to pick it up, but it was too high up in the sky. \n",
            "Lily's mommy came outside and saw the rock. She said, \"Be careful, Lily. It's too dangerous.\" \n",
            "Lily was sad because she really wanted to pick the rock. She asked her mommy, \"Can I pick it up?\" \n",
            "Her mommy said, \"No, Lily. You can't pick it up. It's too dangerous.\" \n",
            "Lily was sad but she understood. She said, \"I want to pick the rock, but I can't pick it.\" \n",
            "Her mommy said, \"Don't worry, Lily. I'll help you pick it up.\" \n",
            "Lily was so happy and said, \"Thank you, mommy. I will pick it up and you can pick it up.\" \n",
            "From that day on, Lily and her mommy played together every day.\n",
            "\n",
            "\n",
            "Step 9400, Loss: 1.394408106803894, Elapsed Time: 17.05 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her favorite toy was a teddy bear. One day, Lily's mommy told her that she was going to have a special treat. Lily was so excited to see what she could do with her bear.\n",
            "When they got to the store, Lily saw a big, shiny toy that she really wanted. She asked her mommy if she could have it, but her mommy said no. Lily was sad, but she didn't want to share her toy with her.\n",
            "Later that day, Lily's friend came over to play. Her friend asked her if she wanted to play with her toy bear. Lily said yes and they played together all day. When it was time to go home, Lily said goodbye to her friend and promised to play with her toy bear again.\n",
            "\n",
            "\n",
            "Step 9600, Loss: 1.432531714439392, Elapsed Time: 16.49 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she went to the park with her mom and saw a big, red ball. She wanted to play with it, but her mom said no.\n",
            "Lily was sad because she really wanted to play with the ball. She asked her mom if she could play with it, but her mom said no. Lily was sad and didn't know what to do. She asked her mom if she could play with the ball. Her mom said yes, and they played together.\n",
            "After playing, Lily played with the ball and it was time to go home. She was so happy that she had played with the ball all day long. She knew that she could always count on her mom and she was happy too.\n",
            "\n",
            "\n",
            "Step 9800, Loss: 1.4160338640213013, Elapsed Time: 16.45 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she went on a walk with her mommy and daddy. They walked and walked until they came to a big tree. Lily was so excited and wanted to climb the tree.\n",
            "Her mommy said, \"Be careful, Lily. That tree is very high.\"\n",
            "Lily nodded and said, \"I will be careful. I will be careful.\"\n",
            "Her mommy smiled and said, \"Okay, Lily. You can climb the tree and climb the tree.\"\n",
            "Lily was so happy and she started to climb the tree. She was so excited and she couldn't wait to go down the tree.\n",
            "\n",
            "\n",
            "Step 10000, Loss: 1.3985328674316406, Elapsed Time: 16.44 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she went to the park with her mom and saw a big tree. She wanted to climb it, but she was scared.\n",
            "Lily's mom said, \"Don't worry, Lily. I'll help you climb the tree and see if you can climb the tree.\"\n",
            "Lily was so excited and said, \"Okay, mommy. I'll climb the tree and climb the tree.\"\n",
            "After a few minutes, Lily felt so happy and proud of herself. She said, \"Thank you, mommy! I love the tree!\"\n",
            "\n",
            "\n",
            "Step 10200, Loss: 1.4001164436340332, Elapsed Time: 16.57 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she went to the park with her mom.\n",
            "At the park, Lily saw a big tree. She wanted to climb it. She asked her mom if she could. Her mom said yes, but she had to be careful.\n",
            "Lily started to climb the tree. She was very careful and careful not to fall. She was careful and didn't hurt herself. She was careful and didn't hurt herself.\n",
            "After a while, Lily got tired and went to bed. She dreamed of playing with her toys and playing with her toys. She dreamed of playing with her toys and having fun.\n",
            "\n",
            "\n",
            "Step 10400, Loss: 1.4185175895690918, Elapsed Time: 16.26 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she found a big, shiny rock in the grass. She was so excited and wanted to show it to her mom.\n",
            "Lily's mom said, \"Be careful, Lily. It's a big rock.\" But Lily didn't listen. She kept on playing and exploring the rock. Suddenly, she saw a big rock on the rock. She wanted to see what was inside.\n",
            "Lily's mom said, \"Be careful, Lily. It might be dangerous.\" But Lily didn't listen. She kept on playing and exploring the rock. Suddenly, she heard a loud noise. It was a big dog! The dog was barking and running. Lily was scared and ran away.\n",
            "Lily's mom saw what happened and ran to the rock. She said, \"Lily, that was dangerous. You should have listened to me.\" Lily's mom said, \"I'm sorry, Lily. I didn't know you are safe.\"\n",
            "Lily's mom said, \"It's okay, Lily. I'm sorry, but you should have listened to me. I know you're not listening to the rock.\"\n",
            "L!!!!\n",
            "\n",
            "Step 10600, Loss: 1.4095873832702637, Elapsed Time: 17.15 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her favorite was a big box. One day, she found a box in her room. She opened it and found a box full of colorful toys. She was so happy and excited to play with them.\n",
            "Lily opened the box and found a toy car inside. She was so happy to play with it. She played with the car all day and had so much fun. When it was time to go home, Lily's mom said it was time to go to bed. Lily was sad because she couldn't play with her toys anymore.\n",
            "\n",
            "\n",
            "Step 10800, Loss: 1.4013723134994507, Elapsed Time: 16.24 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and run around outside. One day, she went to the park with her mommy and saw a big slide. She wanted to go down the slide, but her mommy said no because it was too high for her to go.\n",
            "Lily was sad and didn't know what to do. She asked her mommy if she could go down the slide. Her mommy said yes and they went down the slide together.\n",
            "As they were walking down the slide, Lily saw a big, scary dog. She was scared and didn't know what to do. But then, she remembered her mommy telling her that it's important to be safe and not to be afraid of the dog.\n",
            "Lily learned that it's important to be safe and not to go too far from others. She also learned that it's important to be safe and to be safe.\n",
            "\n",
            "\n",
            "Step 11000, Loss: 1.4003640413284302, Elapsed Time: 16.75 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, bright rainbow in the sky. It was so beautiful!\n",
            "Lily wanted to see the rainbow, so she asked her mommy for help. Her mommy said, \"Let's go find the rainbow and see it.\" Lily was so excited and couldn't wait to see the rainbow.\n",
            "As they were walking, Lily saw a beautiful rainbow in the sky. She asked her mommy, \"Can I see it?\" Her mommy replied, \"Sure, sweetie. But be careful not to get too close.\" Lily was so happy and excited to see the rainbow. She ran around the rainbow and saw the rainbow. She was so happy and thanked her mommy for helping her. From that day on, Lily always remembered to be careful when exploring the rainbow.\n",
            "\n",
            "\n",
            "Step 11200, Loss: 1.4015861749649048, Elapsed Time: 16.69 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she found a shiny rock and decided to take it home with her.\n",
            "Lily's mom saw her and said, \"Lily, it's time to take a break. You need to be careful and not touch it.\" Lily didn't want to listen and started to feel scared.\n",
            "Lily's mom told her, \"If you don't touch the rock, you might hurt yourself.\" Lily felt sad and wanted to keep playing. But then, she remembered that her mom had told her to be careful and not touch things that belong to her.\n",
            "Lily learned her lesson and promised to be more careful with her things. From that day on, Lily always listened to her mom and never touched anything she wanted to.\n",
            "\n",
            "\n",
            "Step 11400, Loss: 1.3889188766479492, Elapsed Time: 16.60 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she found a shiny rock and decided to take it home with her.\n",
            "As she was walking home, she saw a big, shiny rock. She wanted to take it home with her, but it was too expensive. So, she decided to take it home and show it to her mom.\n",
            "When Lily got home, she showed her mom the rock and said it was very special. Her mom was very happy and said it was a special rock. Lily was so excited to show it to her mom.\n",
            "From then on, Lily always wore her shiny rock and showed it to all her friends. She was very proud of herself for taking care of it and showing it to all her friends.\n",
            "\n",
            "\n",
            "Step 11600, Loss: 1.4106708765029907, Elapsed Time: 16.50 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and run around outside. One day, she found a big, shiny rock in her garden. She was so happy and excited to show her friends.\n",
            "Lily's mom saw her and said, \"Lily, it's time to go to the park. You need to be careful and stay away from the rock.\" Lily nodded and promised to be more careful.\n",
            "As she was walking, she saw a big, shiny rock. She picked it up and showed it to her friends. They were so impressed with her and they all wanted to play with it too. Lily was so happy to have her friends back and she felt proud of herself for being so careful with her.\n",
            "From that day on, Lily always remembered to be careful with her toys and to always be careful with her toys.\n",
            "\n",
            "\n",
            "Step 11800, Loss: 1.408809781074524, Elapsed Time: 16.63 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the rain. One day, she went to the park to play with her friends. They were having so much fun!\n",
            "Suddenly, Lily saw a big puddle of mud. She ran to it and jumped into it. She splashed and laughed as she splashed around. She was so happy that she started to jump and play in the mud.\n",
            "Suddenly, Lily heard a loud noise. It was her mommy! She ran to her and hugged her. She said, \"Don't worry, Lily. I'll help you.\" She picked up her favorite toy and put it in the mud.\n",
            "After a while, Lily's mommy came back to her. She said, \"Thank you for helping me clean my mud. You're such a good girl.\" Lily smiled and said, \"You're welcome, my mommy. I'm glad you're safe.\"\n",
            "\n",
            "\n",
            "Step 12000, Loss: 1.4156877994537354, Elapsed Time: 16.76 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she was walking in the park when she saw a big tree. She wanted to climb it, but she was scared.\n",
            "Suddenly, a voice called out, \"Come on, Lily! I'm a friendly bird!\"\n",
            "Lily was a little scared, but she was also curious. She asked the bird, \"What are you doing here?\"\n",
            "The bird replied, \"I'm here to help you climb the tree. I'm here to help you climb the tree.\"\n",
            "Lily was so excited and said, \"Thank you, bird! I'm here to help you climb the tree.\"\n",
            "The bird flew down and landed on Lily's shoulder. She was so happy and grateful for the friendly bird's help. From that day on, Lily always remembered to be kind and help others.\n",
            "\n",
            "\n",
            "Step 12200, Loss: 1.4113636016845703, Elapsed Time: 16.69 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lucy. She was very curious and loved to explore. One day, Lucy was walking in the park when she saw a big, scary dog. The dog was so scared that Lucy was so scared that she ran away.\n",
            "Lucy was so scared that she ran away. She ran back to her house and told her mom about the scary dog. Her mom was very worried and asked her what was wrong. Lucy told her mom about the scary dog. Her mom was very worried and asked her what was wrong.\n",
            "Her mom told her that the scary dog was scared and that she was scared. Lucy felt better and went back to her house. She was so relieved that she was safe.\n",
            "The moral of the story is that it's okay to be scared, but it's okay to be scared.\n",
            "\n",
            "\n",
            "Step 12400, Loss: 1.421187400817871, Elapsed Time: 16.62 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she found a shiny rock and decided to take it home.\n",
            "As she was walking home, she saw a big, shiny rock. She wanted to take it home and show her mommy. But when she got home, she realized that she had lost her balance and fell. She was very sad and didn't know what to do.\n",
            "Lily's mommy saw what happened and came to see what was wrong. She asked what happened and Lily told her about the rock. Her mommy was very angry and told her that she should not take things that don't belong to her. Lily felt very sorry and promised to never take things that didn't belong to her.\n",
            "From that day on, Lily learned to be careful and not take things that didn't belong to her. She also learned that it's important to take care of her and to always take care of her belongings.\n",
            "\n",
            "\n",
            "Step 12600, Loss: 1.3917516469955444, Elapsed Time: 16.81 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lucy. She was very excited because today was a special day. She was going to go to the park with her mom.\n",
            "When they arrived, Lucy saw a big slide. She wanted to go on it, so she asked her mom if she could go on it. Her mom said yes, and Lucy ran to the slide.\n",
            "When they got to the slide, Lucy saw a big, shiny slide. She wanted to go on it, but her mom said no. Lucy was sad, but she was still sad.\n",
            "Then, Lucy saw a big, shiny thing. She wanted to try it. So, she asked her mom if she could go on the slide. Her mom said yes, and Lucy was so excited.\n",
            "The next day, Lucy and her mom went to the park. Lucy was so happy! She had a lot of fun on the slide.\n",
            "\n",
            "\n",
            "Step 12800, Loss: 1.3863184452056885, Elapsed Time: 16.70 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she went to the park with her mom. They saw a big slide and wanted to go on it.\n",
            "Lily's mom said, \"Let's go on the slide and slide down the slide. It looks so fun!\"\n",
            "Lily was so excited and said, \"Yes, let's go!\"\n",
            "They went down the slide and it was so much fun. They laughed and played until it was time to go home. Lily said, \"Mommy, I had so much fun today. I'm so glad I got to go on the slide and slide down the slide!\"\n",
            "\n",
            "\n",
            "Step 13000, Loss: 1.392318844795227, Elapsed Time: 16.54 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she decided to go on an adventure.\n",
            "As she walked, she saw a big tree with a lot of leaves. She wanted to climb it, but she was scared. She asked her mom if she could climb the tree.\n",
            "Her mom said, \"No, Lily. It's too dangerous. You might hurt yourself.\"\n",
            "But Lily didn't listen. She climbed the tree and started to climb. She was so excited that she started to climb the tree. But then, she slipped and fell. She hurt her knee and started to cry.\n",
            "Her mom came over and saw what happened. She said, \"Don't worry, Lily. I will help you. Let's go home and get some ice cream.\"\n",
            "Lily was so happy and thanked her mom for helping her. From that day on, she always remembered to listen to her mom and never climbed the tree again.\n",
            "\n",
            "\n",
            "Step 13200, Loss: 1.403260588645935, Elapsed Time: 16.64 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lucy. She was three years old and loved to explore. One day, Lucy was playing in her backyard when she saw a big, scary spider. She was scared and wanted to run away.\n",
            "Lucy's mom came running and said, \"Don't worry Lucy, I will protect you. Let's go and look for the spider.\"\n",
            "Lucy was scared but she was brave. She said, \"Okay, mom. I will protect you.\"\n",
            "So Lucy and her mom went to the backyard and found the spider hiding behind a tree. Lucy was so happy and said, \"Mom, I'm so brave. I'm so brave!\"\n",
            "Her mom smiled and said, \"That's great, Lucy. You are a brave girl. You are a brave girl.\"\n",
            "Lucy smiled and said, \"Thank you, Mom. I'm so glad I could protect you.\"\n",
            "\n",
            "\n",
            "Step 13400, Loss: 1.3559054136276245, Elapsed Time: 16.61 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore. One day, she found a big, shiny rock and decided to take it home.\n",
            "When she got home, she showed her mom the rock and said, \"Look, mommy! I found a rock!\" Her mom smiled and said, \"That's a great rock, Lily! It's very useful.\"\n",
            "Later that day, Lily went to the park with her mom. She saw a big, shiny rock and wanted to take it home. She asked her mom, \"Can I take it home?\" Her mom said, \"No, Lily. It's not safe.\"\n",
            "Lily was sad, but she knew her mom was right. She put the rock back where it belonged. She went home and told her mom about the rock. Her mom was very proud of her for being so helpful.\n",
            "\n",
            "\n",
            "Step 13600, Loss: 1.3700141906738281, Elapsed Time: 16.67 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she was playing in the park when she saw a big, scary dog. She was scared and didn't know what to do.\n",
            "Lily asked the dog, \"What's wrong?\" The dog replied, \"I'm scared of the dog and I don't know how to get home.\" Lily was scared but she didn't know what to do.\n",
            "Suddenly, the dog started barking and Lily was scared. She didn't know what to do. She tried to run away, but the dog was too fast. Lily was very sad and scared. She cried and cried, but no one heard her.\n",
            "Sadly, the dog never came to the park again. Lily never found her home and never found her home.\n",
            "\n",
            "\n",
            "Step 13800, Loss: 1.3677730560302734, Elapsed Time: 16.65 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she found a shiny rock and decided to bury it in the ground. She dug a hole and put the rock inside.\n",
            "As she was burying the rock, she heard a voice. It was a little bird. \"What are you doing?\" asked Lily.\n",
            "\"I'm burying the rock,\" said the bird.\n",
            "Lily was very curious. She wanted to see what was inside the rock. She found a shiny rock and put it inside the rock.\n",
            "\"Wow!\" said Lily.\n",
            "\"That's a great idea!\" said the bird.\n",
            "Lily was so happy. She put the rock inside the rock and buried it. She was very proud of herself.\n",
            "\n",
            "\n",
            "Step 14000, Loss: 1.3609939813613892, Elapsed Time: 16.36 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she found a big, round rock. She was so excited that she wanted to show it to her friends. \n",
            "Lily's mom said, \"Let's go to the park and see what we can find.\" Lily was so happy that she ran to the park and found a big pile of leaves. She put them in the pile and started to jump around. \n",
            "When she was done, she showed her friends the big, round leaves. They were so happy and excited to see them again. Lily was so proud of her new friends. She showed them the big, round leaves and they all had a great time. \n",
            "From that day on, Lily and her friends would play in the park every day. They would play and have lots of fun together, and Lily was always happy to have found such a big, round and round and round.\n",
            "\n",
            "\n",
            "Step 14200, Loss: 1.3363205194473267, Elapsed Time: 16.55 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she decided to go for a walk in the woods. As she was walking, she saw a big, scary wolf. She was so scared that she started to cry.\n",
            "Suddenly, a big wolf appeared and scared her. She ran away and hid behind a tree. The wolf was so scared that he ran away. Lily was so scared that she ran away.\n",
            "The wolf chased after her and ran away. Lily was so relieved that she had been safe. She was so relieved that she had been safe and safe. From that day on, Lily always remembered to be safe and safe.\n",
            "\n",
            "\n",
            "Step 14400, Loss: 1.3705015182495117, Elapsed Time: 16.27 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she was playing with her toys when she heard a loud noise. She looked around and saw a big truck coming towards her.\n",
            "Lily's mom saw her crying and asked, \"What's wrong, Lily?\"\n",
            "\"My mommy said, \"I'm so angry. I'm so angry and I'm so mad that I didn't know what to do.\"\n",
            "\"Don't worry, Lily. I'll help you,\" her mommy said.\n",
            "Lily was so happy and she hugged her mommy. She said, \"Thank you, Mommy. I love playing with you.\"\n",
            "From that day on, Lily always remembered to be kind to others. She was happy to have her toys back and to play with her toys.\n",
            "\n",
            "\n",
            "Step 14600, Loss: 1.404930591583252, Elapsed Time: 16.49 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she found a big, shiny rock and decided to take a closer look.\n",
            "As she was walking, she saw a big, shiny rock. She wanted to take it home and show her mommy. So, she decided to take the rock home and show her mommy.\n",
            "When she got home, her mommy showed her the rock and told her to take it home. Lily was so happy and excited to show her mommy her new rock.\n",
            "From that day on, Lily always remembered to take care of her rock and take care of it. She was very careful and always made sure to take care of it.\n",
            "Final generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she went to the park with her mommy and daddy. They saw a big tree and Lily wanted to climb it.\n",
            "Lily's mommy said, \"Be careful, Lily. Don't go too high.\" Lily nodded her head and went to the tree. She was so happy to be outside.\n",
            "As they were climbing, Lily saw a big, scary dog. She was scared and ran away. But her mommy and daddy said, \"Don't worry, Lily. We can protect you.\" They took her to a safe place and protected her. Lily felt safe and happy.\n",
            "From that day on, Lily always remembered to be careful when climbing trees. She knew that she was safe and protected her mommy from the scary dog.\n"
          ]
        }
      ],
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n",
        "metrics = nnx.MultiMetric(\n",
        "    loss=nnx.metrics.Average(\"loss\"),\n",
        ")\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"Once upon a time\"\n",
        "start_tokens = tokenizer.encode(start_prompt)[:maxlen]\n",
        "print(\"Initial generated text:\")\n",
        "generated_text = model.generate_text(maxlen, start_tokens)\n",
        "\n",
        "metrics_history = {\n",
        "    \"train_loss\": [],\n",
        "}\n",
        "\n",
        "prep_target_batch = jax.vmap(\n",
        "    lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0])))\n",
        ")\n",
        "\n",
        "step = 0\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    for batch in text_dl:\n",
        "        if len(batch) % len(jax.devices()) != 0:\n",
        "            continue  # skip the remaining elements\n",
        "        input_batch = jnp.array(jnp.array(batch).T)\n",
        "        target_batch = prep_target_batch(input_batch)\n",
        "        train_step(\n",
        "            model,\n",
        "            optimizer,\n",
        "            metrics,\n",
        "            jax.device_put(\n",
        "                (input_batch, target_batch), NamedSharding(mesh, P(\"batch\", None))\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        if (step + 1) % 200 == 0:\n",
        "            for metric, value in metrics.compute().items():\n",
        "                metrics_history[f\"train_{metric}\"].append(value)\n",
        "            metrics.reset()\n",
        "\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(\n",
        "                f\"\\n\\nStep {step + 1}, Loss: {metrics_history['train_loss'][-1]}, Elapsed Time: {elapsed_time:.2f} seconds\"\n",
        "            )\n",
        "            start_time = time.time()\n",
        "\n",
        "            print(\"Generated text:\")\n",
        "            generated_text = model.generate_text(maxlen, start_tokens)\n",
        "\n",
        "        step += 1\n",
        "\n",
        "# Final text generation\n",
        "print(\"Final generated text:\")\n",
        "generated_text = model.generate_text(maxlen, start_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thaLs6TD0lt5"
      },
      "source": [
        "Visualize the training loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "B6Eg1Cz2y_iP",
        "outputId": "5da679f6-44de-443d-e73f-a6c35909a089"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(metrics_history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB-ExEt1Zl1C"
      },
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sensible tiny stories at the end of the training. So essentially we have pretrained a small LLM to write tiny stories for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soPqiR1JNmjf"
      },
      "source": [
        "## Saving the checkpoint\n",
        "\n",
        "Save the model checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkoFGCgSZ1yz",
        "outputId": "3467b8ba-ce05-42f0-fb89-75922cc91e31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_CHECKPOINT_METADATA  d  manifest.ocdbt  _METADATA  ocdbt.process_0  _sharding\n"
          ]
        }
      ],
      "source": [
        "import orbax.checkpoint as orbax\n",
        "\n",
        "state = nnx.state(model)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "checkpointer.save('/content/save', args=orbax.args.PyTreeSave(state), force=True)\n",
        "\n",
        "# Make sure the files are there\n",
        "!ls /content/save/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3813cbf2"
      },
      "source": [
        "## Profiling for hyperparameter tuning\n",
        "\n",
        "**Note:** this section assume multiple TPU cores. Free-tier Colab TPU v5e-1 cannot run here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5d933c6"
      },
      "outputs": [],
      "source": [
        "!pip install -Uq tensorboard-plugin-profile tensorflow tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ac5fc4d"
      },
      "source": [
        "Load the tensorboard colab extension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74f0c212"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17c6131f"
      },
      "source": [
        "As we're going to be running this model a number of times, we need some scaffolding to more easily compare our work. For a baseline, we'll need to perform some warmup to guarantee that our code is JIT'd and that our TPUs are warm. For improved comparability, we'll only start tracing after we've finished warmup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddfd576e"
      },
      "outputs": [],
      "source": [
        "trace_dir = \"/tmp/jax-trace/\"\n",
        "\n",
        "def loop_step(batch, step):\n",
        "    input_batch = jnp.array(jnp.array(batch).T)\n",
        "    target_batch = prep_target_batch(input_batch)\n",
        "    train_step(model, optimizer, metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
        "\n",
        "def generate_trace():\n",
        "    tracing_steps = 30\n",
        "    warmup_steps = 5\n",
        "    for current_step in range(warmup_steps + tracing_steps):\n",
        "        if current_step == warmup_steps:\n",
        "            jax.profiler.start_trace(trace_dir)\n",
        "        with jax.profiler.StepTraceAnnotation(\"train\", step_num=current_step):\n",
        "            batch = next(text_dl)\n",
        "            loop_step(batch, current_step)\n",
        "\n",
        "    jax.profiler.stop_trace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de70f5b7"
      },
      "source": [
        "Now we'll perform some traces to compare results of different batch sizes. This will take several minutes as we need to reprocess our input data to prepare new batches each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc9452a6"
      },
      "outputs": [],
      "source": [
        "trace_dir = \"/tmp/jax-trace-batch-comparison/\"\n",
        "\n",
        "batch_size = 64\n",
        "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
        "generate_trace()\n",
        "\n",
        "batch_size = 256\n",
        "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
        "generate_trace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea379965"
      },
      "source": [
        "Run Tensorboard with the Profiler Plugin to compare our runs. Runs are listed in order from newest to oldest, so the top run in the list will be have `batch_size = 256`.\n",
        "\n",
        "The key metrics to focus on here for this hyperparameter are FLOPS Utilization and Average Step Time.\n",
        "\n",
        "In general, we want to maximize FLOPS Utilization while minimizing the step time per training example. In this case, we can see that increasing the batch size from 64 -> 256 achieves both of those. FLOPS increases from 16% to 27%. Average Step Time increase from 100ms to 260ms, however we increased our batch size by 300%. This means we move from 1.5ms per training example to 1.02ms per training example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b86c565a"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=$trace_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "657967a5"
      },
      "source": [
        "Next, we can explore alternative parallelism methods. In cell #4, we used 4-way data parallel and 2-way tensor parallel. 8-way data parallel is another popular way. Let's compare results between them. To switch to 8-way data parallel, we'll replace the `Mesh` definition with:\n",
        "\n",
        "`mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))`\n",
        "\n",
        "JAX will automatically figure out how to shard the model and data to use the new partition strategy and nothing else need to be done. Re-connect the TPU runtime and run it again to see how it runs.\n",
        "\n",
        "How simple and powerful is this! And that's the beauty of JAX automatic parallelism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80daa8dc"
      },
      "outputs": [],
      "source": [
        "trace_dir = \"/tmp/jax-trace-parallelism-comparison/\"\n",
        "\n",
        "mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
        "generate_trace()\n",
        "\n",
        "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))\n",
        "generate_trace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad96e72b"
      },
      "source": [
        "Once again we'll run tensorboard.\n",
        "\n",
        "Looking at the results, we see that the step times are nearly the same, however the FLOPS Utilization is at 13% for 8-way data parallelism compared to 27% or 4-way data parallelism.\n",
        "\n",
        "By looking at the Trace Viewer tool and looking under each TPU's ops, we can see that the TPUs spend a large amount of time idle while waiting for the host, as well as spending a good amount of time in `reduce_sum` operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "780e9c72"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=$trace_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deca486e"
      },
      "source": [
        "By changing hyperparameters and comparing profiles, we're able to gain significant insights into our bottlenecks and limitations. These are just two examples of hyperparameters to tune, but plenty more of them will have significant effects on training speed and resource utilization."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "formats": "ipynb,md:myst"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}